{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path_to_add = \"../gloabl_files/ml_binary_classification_gridsearch_hyperOpt/\"\n",
    "sys.path.append(path_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import subprocess\n",
    "\n",
    "# with open('requirements.txt', 'r') as file:\n",
    "#     packages = file.readlines()\n",
    "\n",
    "# for package in packages:\n",
    "#     package = package.strip()  # Remove leading/trailing whitespaces, if any\n",
    "#     try:\n",
    "#         subprocess.check_call([\"pip\", \"install\", package])\n",
    "#         print(f\"Successfully installed {package}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to install {package}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util import grid_param_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "output = ipw.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "            \n",
    "            'resample' : ['undersample', 'oversample', None],\n",
    "            'scale'    : [True, False],\n",
    "            'feature_n': [100, 95, 75, 50, 25, 5],\n",
    "            'param_space_size':['medium', 'xsmall'],\n",
    "            'n_unique_out': [10],\n",
    "            'outcome_var_n':['1'],\n",
    "                            'percent_missing':[99, 95, 80],  #n/100 ex 95 for 95% # 99.99, 99.5, 9\n",
    "                            'corr':[0.98, 0.85, 0.5, 0.25],\n",
    "                            'data':[{'age':[True, False],\n",
    "                                    'sex':[True, False],\n",
    "                                    'bmi':[True],\n",
    "                                    'ethnicity':[True, False],\n",
    "                                    'bloods':[True, False],\n",
    "                                    'diagnostic_order':[True, False],\n",
    "                                    'drug_order':[True, False],\n",
    "                                    'annotation_n':[True, False],\n",
    "                                    'meta_sp_annotation_n':[True, False],\n",
    "                                    'annotation_mrc_n':[True, False],\n",
    "                                    'meta_sp_annotation_mrc_n':[True, False],\n",
    "                                    'core_02':[False],\n",
    "                                    'bed':[False],\n",
    "                                    'vte_status':[True],\n",
    "                                    'hosp_site':[True],\n",
    "                                    'core_resus':[False],\n",
    "                                    'news':[False],\n",
    "                                    'date_time_stamp':[ False]\n",
    "                                    \n",
    "                                    }]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "space = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [100, 95, 75, 50, 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']),\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [True, False]),\n",
    "        'sex': hp.choice('sex', [True, False]),\n",
    "        'bmi': hp.choice('bmi', [True]),\n",
    "        'ethnicity': hp.choice('ethnicity', [True, False]),\n",
    "        'bloods': hp.choice('bloods', [True, False]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [True, False]),\n",
    "        'drug_order': hp.choice('drug_order', [True, False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [True, False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [True, False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [True, False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [True, False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [True]),\n",
    "        'hosp_site': hp.choice('hosp_site', [True]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False])\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.logger_setup import setup_logger\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('matplotlib.font_manager')\n",
    "\n",
    "# Set the logging level to suppress debug messages\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ml_grid\n",
    "import pathlib\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "\n",
    "from ml_grid.pipeline.data import pipe\n",
    "\n",
    "from ml_grid.util.param_space import ParamSpace\n",
    "\n",
    "base_project_dir_global = 'HFE_ML_experiments/'\n",
    "\n",
    "pathlib.Path(base_project_dir_global).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "st_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "\n",
    "base_project_dir = 'HFE_ML_experiments/' + st_time + \"/\"\n",
    "additional_naming = \"HFE_ML_Grid_\"\n",
    "\n",
    "print(base_project_dir)\n",
    "\n",
    "pathlib.Path(base_project_dir).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "input_csv_path = os.path.join('..', 'gloabl_files', 'ml_binary_classification_gridsearch_hyperOpt', 'notebooks' ,'test_data_hfe_1yr_m_small.csv') #large\n",
    "\n",
    "#init csv to store each local projects results\n",
    "\n",
    "project_score_save_class(base_project_dir)\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "grid_iter_obj = grid_param_space.Grid(sample_n=n_iter).settings_list_iterator\n",
    "\n",
    "\n",
    "def objective(local_param_dict):\n",
    "    clear_output()\n",
    "    #get settings from iterator over grid of settings space\n",
    "    #local_param_dict = next(grid_iter_obj)\n",
    "    print(local_param_dict)\n",
    "    \n",
    "    #init random number string\n",
    "    \n",
    "    idx = random.randint(0,999999999999999999999)\n",
    "\n",
    "    #create object from settings\n",
    "    ml_grid_object = pipe(input_csv_path,\n",
    "                                                drop_term_list=['chrom', 'hfe', 'phlebo'],\n",
    "                                                local_param_dict=local_param_dict,\n",
    "                                                base_project_dir = base_project_dir,\n",
    "                                                additional_naming = additional_naming,\n",
    "                                                test_sample_n = 0,\n",
    "                                                param_space_index = idx\n",
    "                                                \n",
    "                                                )\n",
    "\n",
    "    from ml_grid.pipeline import main\n",
    "\n",
    "    #Example overwrite/append model_class list\n",
    "    #temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "    # ml_grid_object.model_class_list = [h2o_classifier_class(\n",
    "    #             X=ml_grid_object.X_train,\n",
    "    #             y=ml_grid_object.y_train,\n",
    "    #             parameter_space_size=temp_param_space,\n",
    "    #         )]\n",
    "\n",
    "    # Example append \n",
    "    # if(ml_grid_object.time_series_mode == False):\n",
    "    #temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "    #     ml_grid_object.model_class_list.extend([h2o_classifier_class(\n",
    "    #                 X=ml_grid_object.X_train,\n",
    "    #                 y=ml_grid_object.y_train,\n",
    "    #                 parameter_space_size=temp_param_space_size,\n",
    "    #             )])\n",
    "\n",
    "    #pass object to be evaluated and write results to csv\n",
    "    res = main.run(ml_grid_object, local_param_dict=local_param_dict).execute()\n",
    "    \n",
    "    results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')\n",
    "    \n",
    "    highest_metric_from_run = results_df[results_df['i'] == str(idx)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "    \n",
    "    display(results_df[results_df['i'] == str(idx)].sort_values(by='auc').iloc[0])\n",
    "    \n",
    "    result = {\n",
    "        \"loss\": 1-float(highest_metric_from_run),\n",
    "        \"status\": \"ok\"  # Indicate that the evaluation was successful\n",
    "    }\n",
    "    return result\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective(next(grid_iter_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')\n",
    "    \n",
    "# highest_metric_from_run = results_df[results_df['i'] == str(900424809465212743016)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "\n",
    "# highest_metric_from_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials = trials,\n",
    "           verbose=1\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('auc', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.model_classes.nni_sklearn_wrapper import NeuralNetworkClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_param_dict = next(grid_iter_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object = ml_grid.pipeline.data.pipe(input_csv_path,\n",
    "                                                drop_term_list=['chrom', 'hfe', 'phlebo'],\n",
    "                                                local_param_dict=local_param_dict,\n",
    "                                                base_project_dir = base_project_dir,\n",
    "                                                additional_naming = additional_naming,\n",
    "                                                test_sample_n = 0,\n",
    "                                                param_space_index = idx\n",
    "                                                \n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetworkClassifier().fit(ml_grid_object.X_train, ml_grid_object.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(ml_grid_object.X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.run(ml_grid_object, local_param_dict=local_param_dict).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.rmtree('HFE_ML_experiments/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list == column_list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute param grid for all then normalise... for 1:1 for each method\n",
    "\n",
    "#apply max for under util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.model_classes.light_gbm_class import LightGBMClassifierWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LightGBMClassifierWrapper(parameter_space_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.model_classes.quadratic_discriminant_class import \\\n",
    "    quadratic_discriminant_analysis_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd = quadratic_discriminant_analysis_class(\n",
    "                X=ml_grid_object.X_train, y=ml_grid_object.y_train, parameter_space_size=parameter_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_space_size = 'medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm = LightGBMClassifierWrapper(X=ml_grid_object.X_train, y=ml_grid_object.y_train,\n",
    "                                     parameter_space_size=parameter_space_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm.algorithm_implementation.fit(ml_grid_object.X_train, ml_grid_object.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_algorithm = lgbm.algorithm_implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter_v = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_n_jobs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = lgbm.parameter_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import (classification_report, f1_score, make_scorer,\n",
    "                             matthews_corrcoef, roc_auc_score)\n",
    "from sklearn.model_selection import (GridSearchCV, ParameterGrid,\n",
    "                                     RandomizedSearchCV, RepeatedKFold,\n",
    "                                     cross_validate)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = RandomizedSearchCV(current_algorithm, parameters,\n",
    "                                    verbose=1, cv=[(slice(None), slice(None))],\n",
    "                                    n_jobs =grid_n_jobs, n_iter = n_iter_v, error_score=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(ml_grid_object.X_train, ml_grid_object.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.pipeline.data_feature_importance_methods.feature_importance_methods import getNfeaturesANOVAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#100 trials #1258 rows Ã— 52 columns TESYTTTTTTTT WARNING WAS ONLY ON 100 SAMPLE!!!\n",
    "space = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [100, 95, 75, 50, 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']),\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [True, False]),\n",
    "        'sex': hp.choice('sex', [True, False]),\n",
    "        'bmi': hp.choice('bmi', [True]),\n",
    "        'ethnicity': hp.choice('ethnicity', [True, False]),\n",
    "        'bloods': hp.choice('bloods', [True, False]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [True, False]),\n",
    "        'drug_order': hp.choice('drug_order', [True, False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [True, False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [True, False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [True, False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [True, False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [True]),\n",
    "        'hosp_site': hp.choice('hosp_site', [True]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False])\n",
    "    }\n",
    "\n",
    "\n",
    "{'age': 1,\n",
    " 'annotation_mrc_n': 1,\n",
    " 'annotation_n': 1,\n",
    " 'bed': 0,\n",
    " 'bloods': 1,\n",
    " 'bmi': 0,\n",
    " 'core_02': 0,\n",
    " 'core_resus': 0,\n",
    " 'corr': 1,\n",
    " 'date_time_stamp': 0,\n",
    " 'diagnostic_order': 0,\n",
    " 'drug_order': 0,\n",
    " 'ethnicity': 1,\n",
    " 'feature_n': 3,\n",
    " 'hosp_site': 0,\n",
    " 'meta_sp_annotation_mrc_n': 1,\n",
    " 'meta_sp_annotation_n': 1,\n",
    " 'n_unique_out': 0,\n",
    " 'news': 0,\n",
    " 'outcome_var_n': 0,\n",
    " 'param_space_size': 1,\n",
    " 'percent_missing': 1,\n",
    " 'resample': 0,\n",
    " 'scale': 1,\n",
    " 'sex': 1,\n",
    " 'vte_status': 0}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_grid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
