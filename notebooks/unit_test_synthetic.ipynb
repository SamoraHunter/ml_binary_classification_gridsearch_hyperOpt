{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal standalone script to instantiate and test the ml_grid.pipeline.data.pipe class.\n",
    "\n",
    "This script provides a clear example of the minimum setup required to create\n",
    "an `ml_grid_object`. It is intended for debugging the data pipeline in isolation.\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# --- Essential imports from your ml_grid project ---\n",
    "# Ensure your project is installed (e.g., `pip install -e .`) or the path is configured\n",
    "# so these imports can be found.\n",
    "try:\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components.\")\n",
    "    print(\"Please ensure you are running this script from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SETUP ENVIRONMENT & PATHS\n",
    "# =============================================================================\n",
    "print(\"1. Setting up environment and paths...\")\n",
    "\n",
    "# Assume the script is run from the project's root directory\n",
    "project_root = Path().resolve()\n",
    "base_project_dir = str(project_root)\n",
    "print(f\"  Project Root: {project_root}\")\n",
    "\n",
    "# Define the path to your input data.\n",
    "# Using Path() ensures the variable is a Path object, not a string.\n",
    "input_csv_path = Path(\"test_data_hfe_1yr_m_small_multiclass.csv\")\n",
    "\n",
    "# This check will now work correctly because input_csv_path is a Path object.\n",
    "if not input_csv_path.exists():\n",
    "    print(f\"  ERROR: Data file not found at '{input_csv_path}'\")\n",
    "    print(\"  Please make sure the path is correct.\")\n",
    "    exit()\n",
    "print(f\"  Input CSV: {input_csv_path}\")\n",
    "\n",
    "# Create a directory for this specific experiment run's logs and outputs\n",
    "experiments_base_dir = project_root / \"experiments\"\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=experiments_base_dir,\n",
    "    additional_naming=\"PipeDebug\"\n",
    ")\n",
    "print(f\"  Experiment Directory: {experiment_dir}\")\n",
    "\n",
    "# Configure global parameters (optional, but good practice)\n",
    "global_parameters.verbose = 2  # Set to 0 for silent, 1 for info, 2 for debug\n",
    "global_parameters.error_raise = True # Set to True to stop on errors\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. DEFINE PIPELINE PARAMETERS\n",
    "# =============================================================================\n",
    "print(\"\\n2. Defining pipeline parameters...\")\n",
    "\n",
    "# --- Define which columns to drop based on substrings ---\n",
    "drop_term_list = ['chrom', 'hfe', 'phlebo']\n",
    "print(f\"  Drop terms: {drop_term_list}\")\n",
    "\n",
    "# --- Define which models to make available for this run ---\n",
    "# This dictionary toggles which model classes will be loaded.\n",
    "# Using simpler names, as the loader will now handle the '_class' suffix.\n",
    "model_class_dict = {\n",
    "    'LogisticRegression': True,\n",
    "    'RandomForestClassifier': True,\n",
    "    'XGB_class': False, # Example of a disabled model\n",
    "    # Add other models as needed, e.g., 'CatBoost', 'LightGBMClassifierWrapper'\n",
    "}\n",
    "print(f\"  Enabled models: {[k for k, v in model_class_dict.items() if v]}\")\n",
    "\n",
    "# --- Define the outcome variable ---\n",
    "# You can override the default outcome variable selection.\n",
    "# Set to None to use the 'outcome_var_n' from local_param_dict.\n",
    "outcome_var = 'outcome_var_1' # Example: explicitly use 'outcome_var_1'\n",
    "print(f\"  Outcome variable override: '{outcome_var}'\")\n",
    "\n",
    "# --- Define the core parameter dictionary for the pipeline ---\n",
    "# This dictionary controls all the data processing steps.\n",
    "local_param_dict = {\n",
    "    'outcome_var_n': 1,  # Default outcome if override is not used (outcome_var_1)\n",
    "    'param_space_size': 'xsmall', # Explicitly set the parameter space size\n",
    "    'scale': True,\n",
    "    'feature_n': 90,     # Select top 80% of features by importance\n",
    "    'use_embedding': False,\n",
    "    'embedding_method': 'pca',\n",
    "    'embedding_dim': 10,\n",
    "    'scale_features_before_embedding': True,\n",
    "    'percent_missing': 98,\n",
    "    'correlation_threshold': 0.95,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    # The 'data' sub-dictionary toggles feature groups on and off.\n",
    "    # These keys correspond to the logic in `get_pertubation_columns`.\n",
    "    'data': {\n",
    "        'age': True,\n",
    "        'sex': True,\n",
    "        'bmi': True,\n",
    "        'ethnicity': True,\n",
    "        'bloods': True,\n",
    "        'diagnostic_order': True, \n",
    "        'drug_order': True,\n",
    "        'annotation_n': True,\n",
    "        'meta_sp_annotation_n': True,\n",
    "        'annotation_mrc_n': True,\n",
    "        'meta_sp_annotation_mrc_n': True,\n",
    "        'core_02': True,\n",
    "        'bed': True,\n",
    "        'vte_status': True,\n",
    "        'hosp_site': True,\n",
    "        'core_resus': True,\n",
    "        'news': True,\n",
    "        'date_time_stamp': False,\n",
    "        'appointments': False,\n",
    "    }\n",
    "}\n",
    "print(\"  Local parameter dictionary configured.\")\n",
    "\n",
    "# A unique index for this parameter combination (useful when iterating)\n",
    "param_space_index = 0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. INSTANTIATE THE PIPE CLASS\n",
    "# =============================================================================\n",
    "print(\"\\n3. Instantiating the 'pipe' class...\")\n",
    "\n",
    "try:\n",
    "    # This is the call to create the ml_grid_object.\n",
    "    # The entire data pipeline runs during this initialization.\n",
    "    ml_grid_object = pipe(\n",
    "        file_name=str(input_csv_path),\n",
    "        drop_term_list=drop_term_list,\n",
    "        local_param_dict=local_param_dict,\n",
    "        base_project_dir=base_project_dir,\n",
    "        experiment_dir=experiment_dir,\n",
    "        test_sample_n=0,  # Use 0 to process the full dataset\n",
    "        param_space_index=param_space_index,\n",
    "        model_class_dict=model_class_dict,\n",
    "        outcome_var_override=outcome_var\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS: 'ml_grid_object' created successfully.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 4. INSPECT THE RESULTS\n",
    "    # =========================================================================\n",
    "    print(\"\\n4. Inspecting the final object attributes:\")\n",
    "    print(f\"  - Outcome Variable Used: {ml_grid_object.outcome_variable}\")\n",
    "    print(f\"  - X_train shape: {ml_grid_object.X_train.shape}\")\n",
    "    print(f\"  - y_train shape: {ml_grid_object.y_train.shape}\")\n",
    "    print(f\"  - X_test shape: {ml_grid_object.X_test.shape}\")\n",
    "    print(f\"  - y_test shape: {ml_grid_object.y_test.shape}\")\n",
    "    print(f\"  - Number of final features: {len(ml_grid_object.final_column_list)}\")\n",
    "    print(f\"  - Number of available models: {len(ml_grid_object.model_class_list)}\")\n",
    "    print(\"\\nFinal X_train columns sample:\")\n",
    "    print(ml_grid_object.X_train.columns.to_list()[:10])\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ERROR: Failed to instantiate the 'pipe' class.\")\n",
    "    print(\"=\"*50)\n",
    "    import traceback\n",
    "    print(f\"\\nAn error of type '{type(e).__name__}' occurred: {e}\")\n",
    "    print(\"\\nFull Traceback:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "# =============================================================================\n",
    "# 4. INITIALIZE AND EXECUTE THE MODEL TRAINING RUN\n",
    "# =============================================================================\n",
    "print(\"\\n4. Initializing and executing the model training run...\")\n",
    "\n",
    "# --- Essential imports ---\n",
    "# Make sure your project is installed or the path is configured.\n",
    "try:\n",
    "    from ml_grid.pipeline.main import run\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components for the 'run' step.\")\n",
    "    print(\"Please ensure you are running this from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Use exit() if running as a script, or just let the error show in a notebook.\n",
    "    # exit() \n",
    "\n",
    "# Check if ml_grid_object exists from the previous cell's execution\n",
    "if 'ml_grid_object' not in locals() or 'local_param_dict' not in locals():\n",
    "    print(\"\\nERROR: 'ml_grid_object' or 'local_param_dict' not found.\")\n",
    "    print(\"Please ensure you have successfully run the previous cell (the 'pipe' instantiation script) first.\")\n",
    "else:\n",
    "    try:\n",
    "        print(\"  Instantiating the 'run' class with the data object...\")\n",
    "        # Instantiate the 'run' class with the object from the data pipeline\n",
    "        run_instance = run(\n",
    "            ml_grid_object=ml_grid_object,\n",
    "            local_param_dict=local_param_dict\n",
    "        )\n",
    "\n",
    "        print(\"  Executing the hyperparameter search and cross-validation...\")\n",
    "        # Execute the hyperparameter search and cross-validation for all models\n",
    "        model_errors, highest_score = run_instance.execute()\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SUCCESS: Model training and evaluation complete.\")\n",
    "        print(\"=\"*50)\n",
    "\n",
    "        # =====================================================================\n",
    "        # 5. INSPECT THE TRAINING RESULTS\n",
    "        # =====================================================================\n",
    "        print(\"\\n5. Inspecting the training results:\")\n",
    "        print(f\"  - Highest score achieved across all models: {highest_score:.4f}\")\n",
    "\n",
    "        if model_errors:\n",
    "            print(f\"\\n  - {len(model_errors)} model(s) encountered errors during training:\")\n",
    "            for i, error_info in enumerate(model_errors):\n",
    "                try:\n",
    "                    # Try to get a meaningful name for the model\n",
    "                    model_name = error_info[0].__class__.__name__\n",
    "                except:\n",
    "                    model_name = \"Unknown Model\"\n",
    "                error_exception = error_info[1]\n",
    "                print(f\"    {i+1}. Model: {model_name}, Error: {error_exception}\")\n",
    "        else:\n",
    "            print(\"\\n  - All configured models ran without critical errors.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ERROR: An unexpected error occurred during the model training run.\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"\\nAn error of type '{type(e).__name__}' occurred: {e}\")\n",
    "        print(\"\\nFull Traceback:\")\n",
    "        traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, display\n",
    "from numpy import absolute, mean, std\n",
    "from pandas.testing import assert_index_equal\n",
    "from sklearn import metrics\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.core import XGBoostError\n",
    "\n",
    "# --- Essential imports from your ml_grid project ---\n",
    "# Ensure your project is installed or the path is configured.\n",
    "try:\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.pipeline.hyperparameter_search import HyperparameterSearch\n",
    "    from ml_grid.util.bayes_utils import calculate_combinations\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "    from ml_grid.util.project_score_save import project_score_save_class\n",
    "    from ml_grid.util.validate_parameters import validate_parameters_helper\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components.\")\n",
    "    print(\"Please ensure you are running this from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Use exit() if running as a script, or just let the error show in a notebook.\n",
    "    # exit()\n",
    "\n",
    "# =============================================================================\n",
    "# STANDALONE SCRIPT TO DEBUG `grid_search_crossvalidate` INTERNALS\n",
    "# =============================================================================\n",
    "\n",
    "# --- 1. Setup Environment & Paths ---\n",
    "print(\"1. Setting up environment and paths...\")\n",
    "# Assume the script is run from the project's root directory\n",
    "project_root = Path().resolve()\n",
    "base_project_dir = str(project_root)\n",
    "print(f\"   Project Root: {project_root}\")\n",
    "\n",
    "# --- Define the path to your input data ---\n",
    "# IMPORTANT: Update this path to the correct location of your file.\n",
    "input_csv_path = Path(\"/home/samorah/_data/ml_binary_classification_gridsearch_hyperOpt/notebooks/test_data_hfe_1yr_m_small_multiclass.csv\")\n",
    "\n",
    "if not input_csv_path.exists():\n",
    "    print(f\"   ERROR: Data file not found at '{input_csv_path}'\")\n",
    "    print(\"   Please make sure the path is correct.\")\n",
    "    # exit()\n",
    "else:\n",
    "    print(f\"   Input CSV: {input_csv_path}\")\n",
    "\n",
    "# Create a directory for this specific experiment run's logs and outputs\n",
    "experiments_base_dir = project_root / \"experiments\"\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=experiments_base_dir,\n",
    "    additional_naming=\"GSCV_Internals_Debug\"\n",
    ")\n",
    "print(f\"   Experiment Directory: {experiment_dir}\")\n",
    "\n",
    "\n",
    "# --- 2. Configure Parameters ---\n",
    "print(\"\\n2. Configuring parameters...\")\n",
    "# Global parameters\n",
    "global_parameters.verbose = 1\n",
    "global_parameters.error_raise = False\n",
    "global_parameters.bayessearch = False\n",
    "global_parameters.random_grid_search = True\n",
    "global_parameters.sub_sample_param_space_pct = 0.2\n",
    "\n",
    "# Local parameters for the data pipeline, configured for your dataset\n",
    "local_param_dict = {\n",
    "    'outcome_var_n': 1,\n",
    "    'param_space_size': 'xsmall',\n",
    "    'scale': True,\n",
    "    'feature_n': 90,\n",
    "    'use_embedding': False,\n",
    "    'percent_missing': 98,\n",
    "    'correlation_threshold': 0.95,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'data': {\n",
    "        'age': True, 'sex': True, 'bmi': True, 'ethnicity': True,\n",
    "        'bloods': True, 'diagnostic_order': True, 'drug_order': True,\n",
    "        'annotation_n': True, 'meta_sp_annotation_n': True,\n",
    "        'annotation_mrc_n': True, 'meta_sp_annotation_mrc_n': True,\n",
    "        'core_02': True, 'bed': True, 'vte_status': True,\n",
    "        'hosp_site': True, 'core_resus': True, 'news': True,\n",
    "        'date_time_stamp': False, 'appointments': False,\n",
    "    }\n",
    "}\n",
    "print(\"   Parameters configured.\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "ml_grid_object = None\n",
    "try:\n",
    "    # --- 3. Run Data Pipeline to Get `ml_grid_object` ---\n",
    "    print(\"\\n3. Initializing data pipeline (`pipe`) to prepare data...\")\n",
    "    ml_grid_object = pipe(\n",
    "        file_name=str(input_csv_path),\n",
    "        drop_term_list=['chrom', 'hfe', 'phlebo'],\n",
    "        experiment_dir=str(experiment_dir),\n",
    "        base_project_dir=base_project_dir,\n",
    "        local_param_dict=local_param_dict,\n",
    "        param_space_index=0,\n",
    "        model_class_dict={'RandomForestClassifier': True},\n",
    "        outcome_var_override='outcome_var_1'\n",
    "    )\n",
    "    print(\"   Data pipeline finished.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- 4. EXPLICIT `grid_search_crossvalidate` INTERNAL LOGIC ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n4. Executing `grid_search_crossvalidate` internal logic...\")\n",
    "    start_time_main = time.time()\n",
    "\n",
    "    # --- 4a. Select a model and extract its properties ---\n",
    "    model_to_test = ml_grid_object.model_class_list[0]\n",
    "    algorithm_implementation = model_to_test.algorithm_implementation\n",
    "    parameter_space = model_to_test.parameter_space\n",
    "    method_name = model_to_test.method_name\n",
    "    print(f\"   - Model for debugging: {method_name}\")\n",
    "\n",
    "    # --- 4b. Initialize variables from `grid_search_crossvalidate.__init__` ---\n",
    "    # CORRECTED: Set each warning filter individually.\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    grid_n_jobs = global_parameters.grid_n_jobs\n",
    "    if \"keras\" in method_name.lower() or \"xgb\" in method_name.lower() or \"catboost\" in method_name.lower():\n",
    "        grid_n_jobs = 1\n",
    "        try:\n",
    "            gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except Exception as e:\n",
    "            print(f\"   - Could not configure GPU for TensorFlow: {e}\")\n",
    "\n",
    "    # Extract data from the ml_grid_object\n",
    "    X_train, y_train = ml_grid_object.X_train, ml_grid_object.y_train\n",
    "    X_test, y_test = ml_grid_object.X_test, ml_grid_object.y_test\n",
    "\n",
    "    # --- 4c. Prepare for Hyperparameter Search ---\n",
    "    max_param_space_iter_value = global_parameters.max_param_space_iter_value\n",
    "    param_grid_size = len(ParameterGrid(parameter_space))\n",
    "    sub_sample_parameter_val = int(global_parameters.sub_sample_param_space_pct * param_grid_size)\n",
    "    n_iter_v = max(2, sub_sample_parameter_val)\n",
    "    n_iter_v = min(n_iter_v, max_param_space_iter_value)\n",
    "    print(f\"   - Hyperparameter search iterations (n_iter): {n_iter_v}\")\n",
    "\n",
    "    # Instantiate the HyperparameterSearch class\n",
    "    search = HyperparameterSearch(\n",
    "        algorithm=algorithm_implementation,\n",
    "        parameter_space=parameter_space,\n",
    "        method_name=method_name,\n",
    "        global_params=global_parameters,\n",
    "        max_iter=n_iter_v,\n",
    "        ml_grid_object=ml_grid_object\n",
    "    )\n",
    "\n",
    "    # --- 4d. Run the Hyperparameter Search ---\n",
    "    print(\"   - Running HyperparameterSearch.run_search()...\")\n",
    "    # This is the core search step (e.g., RandomizedSearchCV.fit())\n",
    "    best_estimator = search.run_search(X_train, y_train)\n",
    "    print(f\"   - Best estimator found: {best_estimator.get_params()}\")\n",
    "\n",
    "    # --- 4e. Fit the Final Model and Evaluate ---\n",
    "    print(\"   - Fitting the best estimator on the full training data...\")\n",
    "    # Use numpy arrays for final fitting\n",
    "    best_estimator.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # --- 4f. Run Cross-Validation on the Best Model ---\n",
    "    print(\"   - Running cross_validate on the best estimator...\")\n",
    "    cv_splitter = RepeatedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
    "    \n",
    "    try:\n",
    "        scores = cross_validate(\n",
    "            best_estimator,\n",
    "            X_train.values,\n",
    "            y_train.values,\n",
    "            scoring=global_parameters.metric_list,\n",
    "            cv=cv_splitter,\n",
    "            n_jobs=grid_n_jobs,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        failed = False\n",
    "        print(\"   - Cross-validation successful.\")\n",
    "        for metric, values in scores.items():\n",
    "            print(f\"     - {metric}: {np.mean(values):.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   - Cross-validation failed: {e}\")\n",
    "        scores = {metric: [0.5] for metric in global_parameters.metric_list} # Default scores\n",
    "        failed = True\n",
    "\n",
    "    # --- 4g. Predict on the Test Set and Calculate Final Score ---\n",
    "    print(\"   - Predicting on the test set...\")\n",
    "    # Use .values to ensure numpy array for prediction\n",
    "    best_pred_orig = best_estimator.predict(X_test.values)\n",
    "    \n",
    "    # The final score to be optimized/reported\n",
    "    final_auc_score = roc_auc_score(y_test, best_pred_orig)\n",
    "    \n",
    "    # --- 4h. Log the results (emulating project_score_save_class) ---\n",
    "    project_score_save_class.update_score_log(\n",
    "        ml_grid_object=ml_grid_object,\n",
    "        scores=scores,\n",
    "        best_pred_orig=best_pred_orig,\n",
    "        current_algorithm=best_estimator,\n",
    "        method_name=method_name,\n",
    "        pg=param_grid_size,\n",
    "        start=start_time_main,\n",
    "        n_iter_v=n_iter_v,\n",
    "        failed=failed\n",
    "    )\n",
    "    print(\"   - Results logged.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- 5. Display the Final Results ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS: Standalone internal logic run complete.\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n   - Model Tested: {method_name}\")\n",
    "    print(f\"   - Final Reported AUC Score on Test Set: {final_auc_score:.4f}\")\n",
    "    print(f\"   - Total execution time: {time.time() - start_time_main:.2f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"A CRITICAL ERROR OCCURRED DURING EXECUTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Error Type: {type(e).__name__}\")\n",
    "    print(f\"Error Message: {e}\")\n",
    "    print(\"\\nFull Traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    # --- 6. Cleanup ---\n",
    "    try:\n",
    "        os.remove('final_grid_score_log.csv')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('experiments')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        \n",
    "    try: \n",
    "        shutil.rmtree('run_0')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    print(\"\\n6.  clean up.\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r HFE_ML_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "# Suppress TensorFlow GPU-related warnings\n",
    "# 0 = all messages are logged (default)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\"\n",
    "warnings.filterwarnings('ignore') \n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag to control CPU core binding\n",
    "limit_cpu_cores = False\n",
    "\n",
    "if limit_cpu_cores:\n",
    "    # Get the current process ID\n",
    "    pid = os.getpid()\n",
    "    print(f\"Notebook PID: {pid}\")\n",
    "\n",
    "    # Define the CPU cores to bind (e.g., cores 0-3)\n",
    "    core_range = \"0-3\"\n",
    "\n",
    "    # Use taskset to bind the current process to specific CPU cores\n",
    "    try:\n",
    "        # Execute taskset command\n",
    "        subprocess.run([\"taskset\", \"-cp\", core_range, str(pid)], check=True)\n",
    "        print(f\"Successfully bound Notebook PID {pid} to CPU cores {core_range}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'taskset' command not found. Please ensure it is installed.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while setting CPU affinity: {e}\")\n",
    "else:\n",
    "    print(\"CPU core binding is disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util import grid_param_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "output = ipw.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "            \n",
    "            'resample' : ['undersample', 'oversample', None],\n",
    "            'scale'    : [True, False],\n",
    "            'feature_n': [100, 95, 75, 50, 25, 5],\n",
    "            'param_space_size':['medium', 'xsmall'],\n",
    "            'n_unique_out': [10],\n",
    "            'outcome_var_n':['1'],\n",
    "                            'percent_missing':[99, 95, 80],  #n/100 ex 95 for 95% # 99.99, 99.5, 9\n",
    "                            'corr':[0.98, 0.85, 0.5, 0.25],\n",
    "                            'data':[{'age':[True, False],\n",
    "                                    'sex':[True, False],\n",
    "                                    'bmi':[True],\n",
    "                                    'ethnicity':[True, False],\n",
    "                                    'bloods':[True, False],\n",
    "                                    'diagnostic_order':[True, False],\n",
    "                                    'drug_order':[True, False],\n",
    "                                    'annotation_n':[True, False],\n",
    "                                    'meta_sp_annotation_n':[True, False],\n",
    "                                    'annotation_mrc_n':[True, False],\n",
    "                                    'meta_sp_annotation_mrc_n':[True, False],\n",
    "                                    'core_02':[False],\n",
    "                                    'bed':[False],\n",
    "                                    'vte_status':[True],\n",
    "                                    'hosp_site':[True],\n",
    "                                    'core_resus':[False],\n",
    "                                    'news':[False],\n",
    "                                    'date_time_stamp':[ False]\n",
    "                                    \n",
    "                                    }]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "space = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [100, 95, 75, 50, 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']),\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'feature_selection_method': hp.choice('feature_selection_method', ['anova', 'markov_blanket']),\n",
    "\n",
    "    # Embedding hyperparameters\n",
    "    'use_embedding': hp.choice('use_embedding', [True, False]),\n",
    "    'embedding_method': hp.choice('embedding_method', ['pca', 'svd']),\n",
    "    'embedding_dim': hp.choice('embedding_dim', [32, 64, 128]),\n",
    "    'scale_features_before_embedding': hp.choice('scale_features_before_embedding', [True, False]),\n",
    "    'cache_embeddings': hp.choice('cache_embeddings', [False]), \n",
    "    \n",
    "    'data': {\n",
    "        'age': hp.choice('age', [True, False]),\n",
    "        'sex': hp.choice('sex', [True, False]),\n",
    "        'bmi': hp.choice('bmi', [True]),\n",
    "        'ethnicity': hp.choice('ethnicity', [True, False]),\n",
    "        'bloods': hp.choice('bloods', [True, False]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [True, False]),\n",
    "        'drug_order': hp.choice('drug_order', [True, False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [True, False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [True, False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [True, False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [True, False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [True]),\n",
    "        'hosp_site': hp.choice('hosp_site', [True]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "        'appointments': hp.choice('appointments', [False])\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug \n",
    "\n",
    "\n",
    "\n",
    "space = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [100, 95, 75, 50, 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']),\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'feature_selection_method': hp.choice('feature_selection_method', ['anova']),\n",
    "\n",
    "    # Embedding hyperparameters\n",
    "    'use_embedding': hp.choice('use_embedding', [ False]),\n",
    "    'embedding_method': hp.choice('embedding_method', ['pca', 'svd']),\n",
    "    'embedding_dim': hp.choice('embedding_dim', [32, 64, 128]),\n",
    "    'scale_features_before_embedding': hp.choice('scale_features_before_embedding', [ False]),\n",
    "    'cache_embeddings': hp.choice('cache_embeddings', [False]), \n",
    "    \n",
    "    'data': {\n",
    "        'age': hp.choice('age', [True, False]),\n",
    "        'sex': hp.choice('sex', [True, False]),\n",
    "        'bmi': hp.choice('bmi', [True]),\n",
    "        'ethnicity': hp.choice('ethnicity', [True]),\n",
    "        'bloods': hp.choice('bloods', [True]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [True]),\n",
    "        'drug_order': hp.choice('drug_order', [True]),\n",
    "        'annotation_n': hp.choice('annotation_n', [True]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [True]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [True]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [True]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [True]),\n",
    "        'hosp_site': hp.choice('hosp_site', [True]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "        'appointments': hp.choice('appointments', [False])\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer sample space:\n",
    "\n",
    "space_breast_cancer = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [ 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']), # Optimise for alternate representations of outcome variable.\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [False]),\n",
    "        'sex': hp.choice('sex', [ False]),\n",
    "        'bmi': hp.choice('bmi', [False]),\n",
    "        'ethnicity': hp.choice('ethnicity', [ False]),\n",
    "        'bloods': hp.choice('bloods', [True, ]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [ False]),\n",
    "        'drug_order': hp.choice('drug_order', [ False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [ False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [ False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [ False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [ False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [False]),\n",
    "        'hosp_site': hp.choice('hosp_site', [False]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally exclude model classes\n",
    "\n",
    "model_class_dict = {\n",
    "        \"LogisticRegression_class\": True,\n",
    "        \"knn_classifiers_class\": True,\n",
    "        \"quadratic_discriminant_analysis_class\": True,\n",
    "        \"SVC_class\": True,\n",
    "        \"XGB_class_class\": True,\n",
    "        \"mlp_classifier_class\": True,\n",
    "        \"RandomForestClassifier_class\": True,\n",
    "        \"GradientBoostingClassifier_class\": True,\n",
    "        \"CatBoost_class\": True,\n",
    "        \"GaussianNB_class\": True,\n",
    "        \"LightGBMClassifierWrapper\": True,\n",
    "        \"adaboost_class\": True,\n",
    "        \"kerasClassifier_class\": True,\n",
    "        \"knn__gpu_wrapper_class\": True,\n",
    "        \"NeuralNetworkClassifier_class\": True,\n",
    "        \"TabTransformer_class\": False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ml_grid\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_FAIL\n",
    "from ml_grid.model_classes.h2o_classifier_class import h2o_classifier_class\n",
    "#from ml_grid.util import create_experiment_directory\n",
    "\n",
    "from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "\n",
    "from ml_grid.pipeline.data import NoFeaturesError, pipe\n",
    "\n",
    "from ml_grid.util.param_space import ParamSpace\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "multiple_outcomes_example = True\n",
    "\n",
    "# Define a base directory and a descriptive name for this experiment batch\n",
    "base_project_dir = 'HFE_ML_experiments'\n",
    "experiment_name = \"HFE_ML_Grid\"\n",
    "n_iter = 5 # Number of parameter combinations to test\n",
    "\n",
    "# --- Setup ---\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=base_project_dir,\n",
    "    additional_naming=experiment_name\n",
    ")\n",
    "\n",
    "if(multiple_outcomes_example == False):\n",
    "    input_csv_path = 'breast_cancer_dataset.csv'\n",
    "\n",
    "else:\n",
    "    input_csv_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "\n",
    "#input_csv_path = os.path.join('..', 'gloabl_files', 'ml_binary_classification_gridsearch_hyperOpt', 'notebooks' ,'test_data_hfe_1yr_m_small.csv') #large\n",
    "\n",
    "#init csv to store each local projects results\n",
    "\n",
    "project_score_save_class(base_project_dir)\n",
    "\n",
    "grid_iter_obj = grid_param_space.Grid(sample_n=n_iter).settings_list_iterator\n",
    "\n",
    "\n",
    "def objective(local_param_dict, outcome_var=None):\n",
    "    clear_output()\n",
    "    #get settings from iterator over grid of settings space\n",
    "    #local_param_dict = next(grid_iter_obj)\n",
    "    print(local_param_dict)\n",
    "    \n",
    "    #init random number string\n",
    "    \n",
    "    idx = random.randint(0,999999999999999999999)\n",
    "\n",
    "    try:\n",
    "        #create object from settings\n",
    "        ml_grid_object = pipe(input_csv_path,\n",
    "                                                    drop_term_list=['chrom', 'hfe', 'phlebo'],\n",
    "                                                    local_param_dict=local_param_dict,\n",
    "                                                    base_project_dir = base_project_dir,\n",
    "                                                    experiment_dir=experiment_dir,  \n",
    "                                                    test_sample_n = 0,\n",
    "                                                    param_space_index = idx,\n",
    "                                                    model_class_dict = model_class_dict,\n",
    "                                                    outcome_var_override = outcome_var\n",
    "                                                    #outcome_var_override = None #override outcome var, example = 'outcome_var_myeloma'\n",
    "                                                    #outcome_var_override = outcome_var_list[outcome_index] # set if multi class ##deprecated\n",
    "                                                    )\n",
    "\n",
    "        from ml_grid.pipeline import main\n",
    "        \n",
    "        \n",
    "        # from ml_grid.model_classes.h2o_classifier_class import h2o_classifier_class\n",
    "\n",
    "        # Example overwrite/append model_class list\n",
    "        # temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "        # ml_grid_object.model_class_list = [h2o_classifier_class(\n",
    "        #             X=ml_grid_object.X_train,\n",
    "        #             y=ml_grid_object.y_train,\n",
    "        #             parameter_space_size=temp_param_space_size,\n",
    "        #         )]\n",
    "\n",
    "        # Example append \n",
    "        # if(ml_grid_object.time_series_mode == False):\n",
    "        #temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "        #     ml_grid_object.model_class_list.extend([h2o_classifier_class(\n",
    "        #                 X=ml_grid_object.X_train,\n",
    "        #                 y=ml_grid_object.y_train,\n",
    "        #                 parameter_space_size=temp_param_space_size,\n",
    "        #             )])\n",
    "\n",
    "        #pass object to be evaluated and write results to csv\n",
    "        errors, highest_score = main.run(ml_grid_object, local_param_dict=local_param_dict).execute()\n",
    "        \n",
    "        log_file_path = Path(base_project_dir) / 'final_grid_score_log.csv'\n",
    "        results_df = pd.read_csv(log_file_path)\n",
    "        \n",
    "        #highest_metric_from_run = results_df[results_df['i'] == str(idx)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "        \n",
    "        highest_metric_from_run = highest_score # for hyperopt multi procesess #AUC\n",
    "        \n",
    "        #display(results_df[results_df['i'] == str(idx)].sort_values(by='auc').iloc[0])\n",
    "        \n",
    "        result = {\n",
    "            \"loss\": 1-float(highest_metric_from_run),\n",
    "            \"status\": \"ok\"  # Indicate that the evaluation was successful\n",
    "        }\n",
    "    except NoFeaturesError as e:\n",
    "        print(f\"Skipping trial due to NoFeaturesError: {e}\")\n",
    "        \n",
    "        print(\"indicies..:\")\n",
    "        print(ml_grid_object.X_train.index)\n",
    "        print(ml_grid_object.y_train.index)\n",
    "        # Return a failure status to hyperopt\n",
    "        return {'status': STATUS_FAIL, 'loss': float('inf')}\n",
    "    \n",
    "    \n",
    "    return result\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "# print all attributes and their values\n",
    "print(vars(global_parameters))\n",
    "\n",
    "if global_parameters.debug_level > 1:\n",
    "        print(\"Debug Mode: Additional logging enabled.\")\n",
    "\n",
    "# Update global parameters\n",
    "#global_parameters.update_parameters(debug_level=0, grid_n_jobs = -1, error_raise = True, max_param_space_iter_value=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "#print all attributes and their values\n",
    "\n",
    "print(vars(global_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun\n",
    "if( multiple_outcomes_example == False):\n",
    "\n",
    "    # Fix the additional argument (outcome_var) using partial\n",
    "    outcome_var = 'outcome_var_1'  # Define your outcome_var\n",
    "    objective_with_outcome = partial(objective, outcome_var=outcome_var)\n",
    "\n",
    "    # Initialize Trials object to store results\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the optimization\n",
    "    best = fmin(\n",
    "        fn=objective_with_outcome,  # Use the partial function\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=1,\n",
    "        trials=trials,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    results_df = pd.read_csv(os.path.join(base_project_dir, 'final_grid_score_log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    results_df.sort_values('auc', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    results_df.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == True):\n",
    "    \n",
    "    dft = pd.read_csv('test_data_hfe_1yr_m_small_multiclass.csv', nrows=1)\n",
    "    dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outcome variables by finding prefix \"outcome_var_\" in column list\n",
    "\n",
    "if( multiple_outcomes_example == True):\n",
    "    outcome_var_list = [dft.columns[i] for i in range(len(dft.columns)) if \"outcome_var_\" in dft.columns[i]]\n",
    "\n",
    "    outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outcomes one vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun\n",
    "if multiple_outcomes_example == True:\n",
    "    \n",
    "    import multiprocessing\n",
    "    from datetime import datetime\n",
    "    from hyperopt import fmin, tpe, Trials\n",
    "    from joblib import Parallel, delayed\n",
    "    from joblib import parallel_backend\n",
    "    import traceback\n",
    "    import sys\n",
    "\n",
    "    # Get number of cores\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "    def process_single_outcome(outcome_index, outcome_var_list):\n",
    "        \"\"\"Process a single outcome index using multiprocessing.\"\"\"\n",
    "        outcome_var = outcome_var_list[outcome_index]\n",
    "        start_time = datetime.now()\n",
    "        print(f\"[{start_time}] Starting outcome {outcome_index}: {outcome_var}\")\n",
    "\n",
    "        # Wrap objective to include the outcome_var\n",
    "        def objective_with_outcome(params):\n",
    "            print(f\"Evaluating params: {params} for outcome {outcome_var}\")\n",
    "            return objective(params, outcome_var)\n",
    "\n",
    "        try:\n",
    "            # Use joblib's multiprocessing backend for scikit-learn operations\n",
    "            with parallel_backend('multiprocessing', n_jobs=1):\n",
    "                best = fmin(\n",
    "                    fn=objective_with_outcome,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=1,\n",
    "                    trials=Trials(),\n",
    "                    verbose=0\n",
    "                )\n",
    "            end_time = datetime.now()\n",
    "            print(f\"[{end_time}] Finished outcome {outcome_index} (Duration: {end_time - start_time})\")\n",
    "            return outcome_index, best, None, None\n",
    "       \n",
    "        except Exception as e:\n",
    "            # Capture full traceback\n",
    "            tb_str = traceback.format_exc()\n",
    "            print(f\"Error in outcome {outcome_var}:\\n{tb_str}\", file=sys.stderr)\n",
    "            return outcome_index, None, str(e), tb_str\n",
    "\n",
    "    # Main execution\n",
    "    if __name__ == \"__main__\":\n",
    "        start_total = datetime.now()\n",
    "        print(f\"Starting all optimizations at {start_total}\")\n",
    "\n",
    "        # Use joblib's Parallel with verbose for better error reporting\n",
    "        # Setting verbose=10 will show more details about what's happening\n",
    "        results = Parallel(n_jobs=num_cores, verbose=10)(\n",
    "            delayed(process_single_outcome)(i, outcome_var_list)\n",
    "            for i in range(len(outcome_var_list))\n",
    "        )\n",
    "\n",
    "        # Process results and re-raise first error if any\n",
    "        first_error = None\n",
    "        for outcome_index, best, error, traceback_str in results:\n",
    "            if error:\n",
    "                print(f\"\\n{'='*70}\")\n",
    "                print(f\"Exception on fmin for {outcome_var_list[outcome_index]}:\")\n",
    "                print(f\"{'='*70}\")\n",
    "                print(traceback_str)\n",
    "                if first_error is None:\n",
    "                    first_error = (outcome_var_list[outcome_index], error, traceback_str)\n",
    "            elif best is not None:\n",
    "                print(f\"Best parameters for {outcome_var_list[outcome_index]}: {best}\")\n",
    "            else:\n",
    "                print(f\"No result for {outcome_var_list[outcome_index]}\")\n",
    "\n",
    "        end_total = datetime.now()\n",
    "        print(f\"\\nCompleted all optimizations at {end_total}\")\n",
    "        print(f\"Total duration: {end_total - start_total}\")\n",
    "        \n",
    "        # Re-raise the first error to get a proper stack trace\n",
    "        if first_error:\n",
    "            outcome_name, error_msg, tb_str = first_error\n",
    "            raise RuntimeError(\n",
    "                f\"Error occurred in outcome '{outcome_name}': {error_msg}\\n\\n\"\n",
    "                f\"Original traceback:\\n{tb_str}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the parent directory\n",
    "parent_dir = 'HFE_ML_experiments'\n",
    "\n",
    "# Check if the CSV is directly in the parent directory first\n",
    "csv_path = os.path.join(parent_dir, 'final_grid_score_log.csv')\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    # If not found directly, look for it in timestamped subfolders\n",
    "    # List all folders in the parent directory that match the date pattern\n",
    "    folders = [f for f in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, f))]\n",
    "\n",
    "    # Parse folder names as dates and find the latest one\n",
    "    def parse_date(folder_name: str):\n",
    "        \"\"\"\n",
    "        Parses the timestamp from the beginning of a folder name.\n",
    "        Expected format: 'YYYY-MM-DD_HH-MM-SS_...'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # The timestamp is always the first 19 characters.\n",
    "            timestamp_part = folder_name[:19]\n",
    "            return datetime.strptime(timestamp_part, '%Y-%m-%d_%H-%M-%S')\n",
    "        except (ValueError, IndexError):\n",
    "            # Return None if the folder name doesn't match the expected format or is too short.\n",
    "            return None\n",
    "\n",
    "    # Filter and sort folders by date\n",
    "    folders_with_dates = [(f, parse_date(f)) for f in folders]\n",
    "    folders_with_dates = [f for f in folders_with_dates if f[1] is not None]\n",
    "    \n",
    "    if folders_with_dates:\n",
    "        latest_folder = max(folders_with_dates, key=lambda x: x[1])[0]  # Get the folder with the latest date\n",
    "        print(\"latest_folder\", latest_folder)\n",
    "        \n",
    "        # Construct the path to the CSV file in the latest folder\n",
    "        csv_path = os.path.join(parent_dir, latest_folder, 'final_grid_score_log.csv')\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No timestamped folders found and CSV not in parent directory\")\n",
    "else:\n",
    "    print(\"CSV found directly in parent directory\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sort the DataFrame by 'auc' column in descending order\n",
    "df = df.sort_values(by='auc', ascending=False)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Group by outcome_variable and display the first row of each group with the highest auc\n",
    "df_best = df.groupby('outcome_variable').apply(lambda x: x.iloc[0])\n",
    "\n",
    "# Display the result\n",
    "df_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "experiments_base_dir = Path('HFE_ML_experiments')\n",
    "\n",
    "# --- Find the CSV file (try multiple locations) ---\n",
    "\n",
    "def find_csv_file():\n",
    "    \"\"\"\n",
    "    Search for final_grid_score_log.csv in multiple locations:\n",
    "    1. Project root (parent of experiments_base_dir)\n",
    "    2. Directly in experiments_base_dir\n",
    "    3. In the latest timestamped subfolder\n",
    "    \"\"\"\n",
    "    # Location 1: Project root\n",
    "    project_root = experiments_base_dir.parent\n",
    "    csv_path = project_root / 'final_grid_score_log.csv'\n",
    "    if csv_path.exists():\n",
    "        print(f\"✓ CSV found in project root: {csv_path.resolve()}\")\n",
    "        return csv_path\n",
    "    \n",
    "    # Location 2: Directly in experiments directory\n",
    "    csv_path = experiments_base_dir / 'final_grid_score_log.csv'\n",
    "    if csv_path.exists():\n",
    "        print(f\"✓ CSV found in experiments directory: {csv_path.resolve()}\")\n",
    "        return csv_path\n",
    "    \n",
    "    # Location 3: In latest timestamped subfolder\n",
    "    latest_folder = find_latest_experiment_folder()\n",
    "    if latest_folder:\n",
    "        csv_path = latest_folder / 'final_grid_score_log.csv'\n",
    "        if csv_path.exists():\n",
    "            print(f\"✓ CSV found in latest experiment folder: {csv_path.resolve()}\")\n",
    "            return csv_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_date(folder_name: str):\n",
    "    \"\"\"\n",
    "    Parses the timestamp from the beginning of a folder name.\n",
    "    Expected format: 'YYYY-MM-DD_HH-MM-SS_...'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        timestamp_part = folder_name[:19]\n",
    "        return datetime.strptime(timestamp_part, '%Y-%m-%d_%H-%M-%S')\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_latest_experiment_folder():\n",
    "    \"\"\"Find the most recent timestamped experiment folder.\"\"\"\n",
    "    if not experiments_base_dir.exists() or not experiments_base_dir.is_dir():\n",
    "        print(f\"⚠ Experiments directory not found: {experiments_base_dir.resolve()}\")\n",
    "        return None\n",
    "    \n",
    "    subfolders = [f for f in experiments_base_dir.iterdir() if f.is_dir()]\n",
    "    folders_with_dates = [(f, parse_date(f.name)) for f in subfolders]\n",
    "    valid_folders = [f for f in folders_with_dates if f[1] is not None]\n",
    "    \n",
    "    if valid_folders:\n",
    "        latest_folder = max(valid_folders, key=lambda x: x[1])[0]\n",
    "        print(f\"Latest experiment folder: {latest_folder.name}\")\n",
    "        return latest_folder\n",
    "    else:\n",
    "        print(\"⚠ No valid timestamped experiment folders found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Find the CSV file\n",
    "log_file_path = find_csv_file()\n",
    "\n",
    "if log_file_path:\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(log_file_path)\n",
    "    \n",
    "    # Sort by AUC in descending order\n",
    "    df_sorted = df.sort_values(by='auc', ascending=False)\n",
    "    \n",
    "    print(f\"\\n✓ Successfully loaded {len(df_sorted)} records from the log file.\")\n",
    "    \n",
    "    # Group by outcome_variable and get the best result for each\n",
    "    top_results_by_outcome = df_sorted.groupby('outcome_variable').first().reset_index()\n",
    "    \n",
    "    print(f\"\\nTop results by outcome variable ({len(top_results_by_outcome)} outcomes):\\n\")\n",
    "    \n",
    "    # Display the result\n",
    "    display(top_results_by_outcome)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ Error: Could not find 'final_grid_score_log.csv' in any expected location:\")\n",
    "    print(f\"  - {(experiments_base_dir.parent / 'final_grid_score_log.csv').resolve()}\")\n",
    "    print(f\"  - {(experiments_base_dir / 'final_grid_score_log.csv').resolve()}\")\n",
    "    print(f\"  - In any timestamped subfolder within {experiments_base_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Shape of the dataset: {data.shape}\")\n",
    "print(f\"Columns: {data.columns.tolist()}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for constant features\n",
    "print(\"\\n=== Constant Features ===\")\n",
    "constant_features = [col for col in data.columns if data[col].nunique() == 1]\n",
    "print(f\"Constant features: {constant_features}\")\n",
    "\n",
    "# Check for features with very low variance (almost constant)\n",
    "print(\"\\n=== Low Variance Features ===\")\n",
    "low_variance_features = []\n",
    "for col in data.columns:\n",
    "    if data[col].dtype in [np.float64, np.int64]:  # Check only numeric features\n",
    "        if data[col].std() < 0.01:  # Threshold for low variance\n",
    "            low_variance_features.append(col)\n",
    "print(f\"Low variance features: {low_variance_features}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\n=== Duplicate Rows ===\")\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for class distribution (if it's a classification problem)\n",
    "if 'target' in data.columns:  # Replace 'target' with your actual target column name\n",
    "    print(\"\\n=== Class Distribution ===\")\n",
    "    print(data['target'].value_counts())\n",
    "\n",
    "# Check for categorical features with high cardinality\n",
    "print(\"\\n=== High Cardinality Categorical Features ===\")\n",
    "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
    "high_cardinality_features = [col for col in categorical_features if data[col].nunique() > 100]\n",
    "print(f\"High cardinality categorical features: {high_cardinality_features}\")\n",
    "\n",
    "# Check for outliers in numeric features (using IQR)\n",
    "print(\"\\n=== Outliers in Numeric Features ===\")\n",
    "numeric_features = data.select_dtypes(include=[np.float64, np.int64]).columns\n",
    "for col in numeric_features:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {col}: {len(outliers)} rows\")\n",
    "\n",
    "# Summary of issues\n",
    "print(\"\\n=== Summary of Issues ===\")\n",
    "if missing_values.any():\n",
    "    print(f\"- Missing values found in {missing_values[missing_values > 0].index.tolist()}\")\n",
    "if constant_features:\n",
    "    print(f\"- Constant features found: {constant_features}\")\n",
    "if low_variance_features:\n",
    "    print(f\"- Low variance features found: {low_variance_features}\")\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"- Duplicate rows found: {duplicate_rows}\")\n",
    "if high_cardinality_features:\n",
    "    print(f\"- High cardinality categorical features found: {high_cardinality_features}\")\n",
    "if not missing_values.any() and not constant_features and not low_variance_features and not duplicate_rows and not high_cardinality_features:\n",
    "    print(\"- No major issues found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "\n",
    "# Get the top result for each outcome_variable by AUC\n",
    "top_auc_per_outcome = df.loc[df.groupby('outcome_variable')['auc'].idxmax()]\n",
    "\n",
    "# Sort by AUC for better visualization\n",
    "top_auc_per_outcome = top_auc_per_outcome.sort_values(by='auc', ascending=False)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create barplot to show the top AUC for each outcome_variable\n",
    "sns.barplot(\n",
    "    x='auc', \n",
    "    y='outcome_variable', \n",
    "    data=top_auc_per_outcome, \n",
    "    hue='nb_size', \n",
    "    dodge=False, \n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top AUC for Each Outcome Variable')\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Outcome Variable')\n",
    "plt.legend(title='num features')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes\n",
    "from ml_grid.results_processing.core import ResultsAggregator\n",
    "from ml_grid.results_processing.plot_master import MasterPlotter\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load your data using the ResultsAggregator\n",
    "#    Replace with the actual path to your results and feature names file.\n",
    "#    The feature_names_csv is optional but required for feature-related plots.\n",
    "try:\n",
    "    aggregator = ResultsAggregator(\n",
    "        root_folder='HFE_ML_experiments',\n",
    "        feature_names_csv='test_data_hfe_1yr_m_small_multiclass.csv')\n",
    "    results_df = aggregator.aggregate_all_runs()\n",
    "\n",
    "    # 2. Instantiate the MasterPlotter with your data\n",
    "    master_plotter = MasterPlotter(results_df)\n",
    "\n",
    "    # 3. Call the plot_all() method to generate all visualizations\n",
    "    #    You can customize the primary metric and other options.\n",
    "    master_plotter.plot_all(metric='auc', stratify_by_outcome=True)\n",
    "\n",
    "except (ValueError, FileNotFoundError) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure your results folder path is correct and contains valid run data.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_grid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
