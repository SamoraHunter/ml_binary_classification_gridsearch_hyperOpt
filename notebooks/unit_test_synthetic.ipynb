{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "path_to_add = \"../gloabl_files/ml_binary_classification_gridsearch_hyperOpt/\"\n",
    "sys.path.append(path_to_add)\n",
    "path_to_add = \"../\"\n",
    "sys.path.append(path_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Boolean flag to control CPU core binding\n",
    "limit_cpu_cores = False\n",
    "\n",
    "if limit_cpu_cores:\n",
    "    # Get the current process ID\n",
    "    pid = os.getpid()\n",
    "    print(f\"Notebook PID: {pid}\")\n",
    "\n",
    "    # Define the CPU cores to bind (e.g., cores 0-3)\n",
    "    core_range = \"0-3\"\n",
    "\n",
    "    # Use taskset to bind the current process to specific CPU cores\n",
    "    try:\n",
    "        # Execute taskset command\n",
    "        subprocess.run([\"taskset\", \"-cp\", core_range, str(pid)], check=True)\n",
    "        print(f\"Successfully bound Notebook PID {pid} to CPU cores {core_range}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'taskset' command not found. Please ensure it is installed.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while setting CPU affinity: {e}\")\n",
    "else:\n",
    "    print(\"CPU core binding is disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import subprocess\n",
    "\n",
    "# with open('requirements.txt', 'r') as file:\n",
    "#     packages = file.readlines()\n",
    "\n",
    "# for package in packages:\n",
    "#     package = package.strip()  # Remove leading/trailing whitespaces, if any\n",
    "#     try:\n",
    "#         subprocess.check_call([\"pip\", \"install\", package])\n",
    "#         print(f\"Successfully installed {package}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Failed to install {package}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util import grid_param_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "output = ipw.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "            \n",
    "            'resample' : ['undersample', 'oversample', None],\n",
    "            'scale'    : [True, False],\n",
    "            'feature_n': [100, 95, 75, 50, 25, 5],\n",
    "            'param_space_size':['medium', 'xsmall'],\n",
    "            'n_unique_out': [10],\n",
    "            'outcome_var_n':['1'],\n",
    "                            'percent_missing':[99, 95, 80],  #n/100 ex 95 for 95% # 99.99, 99.5, 9\n",
    "                            'corr':[0.98, 0.85, 0.5, 0.25],\n",
    "                            'data':[{'age':[True, False],\n",
    "                                    'sex':[True, False],\n",
    "                                    'bmi':[True],\n",
    "                                    'ethnicity':[True, False],\n",
    "                                    'bloods':[True, False],\n",
    "                                    'diagnostic_order':[True, False],\n",
    "                                    'drug_order':[True, False],\n",
    "                                    'annotation_n':[True, False],\n",
    "                                    'meta_sp_annotation_n':[True, False],\n",
    "                                    'annotation_mrc_n':[True, False],\n",
    "                                    'meta_sp_annotation_mrc_n':[True, False],\n",
    "                                    'core_02':[False],\n",
    "                                    'bed':[False],\n",
    "                                    'vte_status':[True],\n",
    "                                    'hosp_site':[True],\n",
    "                                    'core_resus':[False],\n",
    "                                    'news':[False],\n",
    "                                    'date_time_stamp':[ False]\n",
    "                                    \n",
    "                                    }]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "space = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [100, 95, 75, 50, 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']),\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'feature_selection_method': hp.choice('feature_selection_method', ['anova', 'markov_blanket']),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [True, False]),\n",
    "        'sex': hp.choice('sex', [True, False]),\n",
    "        'bmi': hp.choice('bmi', [True]),\n",
    "        'ethnicity': hp.choice('ethnicity', [True, False]),\n",
    "        'bloods': hp.choice('bloods', [True, False]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [True, False]),\n",
    "        'drug_order': hp.choice('drug_order', [True, False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [True, False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [True, False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [True, False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [True, False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [True]),\n",
    "        'hosp_site': hp.choice('hosp_site', [True]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "        'appointments': hp.choice('appointments', [False])\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer sample space:\n",
    "\n",
    "space_breast_cancer = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [ 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']), # Optimise for alternate representations of outcome variable.\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [False]),\n",
    "        'sex': hp.choice('sex', [ False]),\n",
    "        'bmi': hp.choice('bmi', [False]),\n",
    "        'ethnicity': hp.choice('ethnicity', [ False]),\n",
    "        'bloods': hp.choice('bloods', [True, ]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [ False]),\n",
    "        'drug_order': hp.choice('drug_order', [ False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [ False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [ False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [ False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [ False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [False]),\n",
    "        'hosp_site': hp.choice('hosp_site', [False]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.logger_setup import setup_logger\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('matplotlib.font_manager')\n",
    "\n",
    "# Set the logging level to suppress debug messages\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally exclude model classes\n",
    "\n",
    "model_class_dict = {\n",
    "        \"LogisticRegression_class\": True,\n",
    "        \"knn_classifiers_class\": True,\n",
    "        \"quadratic_discriminant_analysis_class\": True,\n",
    "        \"SVC_class\": True,\n",
    "        \"XGB_class_class\": True,\n",
    "        \"mlp_classifier_class\": True,\n",
    "        \"RandomForestClassifier_class\": True,\n",
    "        \"GradientBoostingClassifier_class\": True,\n",
    "        \"CatBoost_class\": True,\n",
    "        \"GaussianNB_class\": True,\n",
    "        \"LightGBMClassifierWrapper\": True,\n",
    "        \"adaboost_class\": True,\n",
    "        \"kerasClassifier_class\": True,\n",
    "        \"knn__gpu_wrapper_class\": True,\n",
    "        \"NeuralNetworkClassifier_class\": True,\n",
    "        \"TabTransformer_class\": False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ml_grid\n",
    "import pathlib\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "from ml_grid.model_classes.h2o_classifier_class import h2o_classifier_class\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "\n",
    "from ml_grid.pipeline.data import pipe\n",
    "\n",
    "from ml_grid.util.param_space import ParamSpace\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "\n",
    "base_project_dir_global = 'HFE_ML_experiments/'\n",
    "\n",
    "logger = setup_logger(log_folder_path = base_project_dir_global)\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Add a filter to exclude logs not related to numba.core.byteflow\n",
    "class ByteflowFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return record.name.startswith('numba.core.byteflow')\n",
    "\n",
    "# Add the filter to the logger\n",
    "logger.addFilter(ByteflowFilter())\n",
    "\n",
    "pathlib.Path(base_project_dir_global).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "st_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "\n",
    "base_project_dir = 'HFE_ML_experiments/' + st_time + \"/\"\n",
    "additional_naming = \"HFE_ML_Grid_\"\n",
    "\n",
    "print(base_project_dir)\n",
    "\n",
    "pathlib.Path(base_project_dir).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "input_csv_path = 'breast_cancer_dataset.csv'\n",
    "\n",
    "#input_csv_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "\n",
    "#input_csv_path = os.path.join('..', 'gloabl_files', 'ml_binary_classification_gridsearch_hyperOpt', 'notebooks' ,'test_data_hfe_1yr_m_small.csv') #large\n",
    "\n",
    "#init csv to store each local projects results\n",
    "\n",
    "project_score_save_class(base_project_dir)\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "grid_iter_obj = grid_param_space.Grid(sample_n=n_iter).settings_list_iterator\n",
    "\n",
    "\n",
    "def objective(local_param_dict):\n",
    "    clear_output()\n",
    "    #get settings from iterator over grid of settings space\n",
    "    #local_param_dict = next(grid_iter_obj)\n",
    "    print(local_param_dict)\n",
    "    \n",
    "    #init random number string\n",
    "    \n",
    "    idx = random.randint(0,999999999999999999999)\n",
    "\n",
    "    #create object from settings\n",
    "    ml_grid_object = pipe(input_csv_path,\n",
    "                                                drop_term_list=['chrom', 'hfe', 'phlebo'],\n",
    "                                                local_param_dict=local_param_dict,\n",
    "                                                base_project_dir = base_project_dir,\n",
    "                                                additional_naming = additional_naming,\n",
    "                                                test_sample_n = 0,\n",
    "                                                param_space_index = idx,\n",
    "                                                model_class_dict = model_class_dict,\n",
    "                                                #outcome_var_override = None #override outcome var, example = 'outcome_var_myeloma'\n",
    "                                                #outcome_var_override = outcome_var_list[outcome_index] # set if multi class\n",
    "                                                )\n",
    "\n",
    "    from ml_grid.pipeline import main\n",
    "    \n",
    "    # from ml_grid.model_classes.h2o_classifier_class import h2o_classifier_class\n",
    "\n",
    "    # Example overwrite/append model_class list\n",
    "    # temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "    # ml_grid_object.model_class_list = [h2o_classifier_class(\n",
    "    #             X=ml_grid_object.X_train,\n",
    "    #             y=ml_grid_object.y_train,\n",
    "    #             parameter_space_size=temp_param_space_size,\n",
    "    #         )]\n",
    "\n",
    "    # Example append \n",
    "    # if(ml_grid_object.time_series_mode == False):\n",
    "    #temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "    #     ml_grid_object.model_class_list.extend([h2o_classifier_class(\n",
    "    #                 X=ml_grid_object.X_train,\n",
    "    #                 y=ml_grid_object.y_train,\n",
    "    #                 parameter_space_size=temp_param_space_size,\n",
    "    #             )])\n",
    "\n",
    "    #pass object to be evaluated and write results to csv\n",
    "    errors, highest_score = main.run(ml_grid_object, local_param_dict=local_param_dict).execute()\n",
    "    \n",
    "    \n",
    "    \n",
    "    results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')\n",
    "    \n",
    "    #highest_metric_from_run = results_df[results_df['i'] == str(idx)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "    \n",
    "    highest_metric_from_run = highest_score # for hyperopt multi procesess #AUC\n",
    "    \n",
    "    display(results_df[results_df['i'] == str(idx)].sort_values(by='auc').iloc[0])\n",
    "    \n",
    "    result = {\n",
    "        \"loss\": 1-float(highest_metric_from_run),\n",
    "        \"status\": \"ok\"  # Indicate that the evaluation was successful\n",
    "    }\n",
    "    return result\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective(next(grid_iter_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')\n",
    "    \n",
    "# highest_metric_from_run = results_df[results_df['i'] == str(900424809465212743016)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "\n",
    "# highest_metric_from_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=100,\n",
    "            trials = trials,\n",
    "           verbose=1\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('auc', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dft = pd.read_csv('test_data_hfe_1yr_m_small_multiclass.csv', nrows=1)\n",
    "dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outcome variables by finding prefix \"outcome_var_\" in column list\n",
    "\n",
    "outcome_var_list = [dft.columns[i] for i in range(len(dft.columns)) if \"outcome_var_\" in dft.columns[i]]\n",
    "\n",
    "outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outcomes one vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, TimeoutError, as_completed\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.utils.parallel import delayed, Parallel\n",
    "from joblib import parallel_backend\n",
    "\n",
    "def process_single_outcome(outcome_index, outcome_var_list):\n",
    "    \"\"\"Process a single outcome index\"\"\"\n",
    "    outcome_var = outcome_var_list[outcome_index]\n",
    "    thread_name = threading.current_thread().name\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    print(f\"[{start_time}] Thread: {thread_name} - Starting outcome {outcome_index}: {outcome_var}\")\n",
    "\n",
    "    # Wrap objective to include the outcome_var and handle parallel backend\n",
    "    def objective_with_outcome(params):\n",
    "        print(f\"Thread: {thread_name} - Evaluating params: {params} for outcome {outcome_var}\")\n",
    "        # Use threading backend for scikit-learn operations\n",
    "        with parallel_backend('threading', n_jobs=1):\n",
    "            return objective(params, outcome_var)\n",
    "\n",
    "    try:\n",
    "        # Configure scikit-learn to use threading backend\n",
    "        with parallel_backend('threading', n_jobs=1):\n",
    "            best = fmin(\n",
    "                fn=objective_with_outcome,\n",
    "                space=space,\n",
    "                algo=tpe.suggest,\n",
    "                max_evals=10,\n",
    "                trials=Trials(),\n",
    "                verbose=0\n",
    "            )\n",
    "        end_time = datetime.now()\n",
    "        print(f\"[{end_time}] Thread: {thread_name} - Finished outcome {outcome_index} (Duration: {end_time - start_time})\")\n",
    "        return outcome_index, best, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error in {thread_name} for outcome {outcome_var}: {str(e)}\")\n",
    "        return outcome_index, None, str(e)\n",
    "\n",
    "start_total = datetime.now()\n",
    "print(f\"Starting all optimizations at {start_total}\")\n",
    "\n",
    "# Use threading backend globally\n",
    "with parallel_backend('threading', n_jobs=80):\n",
    "    # Run parallel processing using ThreadPoolExecutor\n",
    "    with ThreadPoolExecutor(max_workers=19) as executor:  # One thread per outcome\n",
    "        futures = [executor.submit(process_single_outcome, i, outcome_var_list) for i in range(19)]\n",
    "\n",
    "        print(f\"Submitted {len(futures)} tasks to parallel executor\")\n",
    "\n",
    "        # Process results as they complete\n",
    "        for future in as_completed(futures):\n",
    "            outcome_index = futures.index(future)\n",
    "            try:\n",
    "                _, best, error = future.result(timeout=360*2)\n",
    "\n",
    "                if error:\n",
    "                    print(f\"Exception on fmin on {outcome_var_list[outcome_index]}: {error}\")\n",
    "                elif best is not None:\n",
    "                    print(f\"Best parameters for {outcome_var_list[outcome_index]}: {best}\")\n",
    "                else:\n",
    "                    print(f\"No result for {outcome_var_list[outcome_index]}\")\n",
    "\n",
    "            except TimeoutError:\n",
    "                print(f\"Timeout reached for {outcome_var_list[outcome_index]}\")\n",
    "\n",
    "end_total = datetime.now()\n",
    "print(f\"\\nCompleted all optimizations at {end_total}\")\n",
    "print(f\"Total duration: {end_total - start_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_grid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
