{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# --- Make sure your notebook's working directory is the project root ---\n",
    "# (e.g., /home/samorah/_data/ml_binary_classification_gridsearch_hyperOpt/)\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from ml_grid.util.impute_data_for_pipe import (\n",
    "    mean_impute_dataframe,\n",
    "    save_missing_percentage,\n",
    ")\n",
    "\n",
    "# so that the imports work correctly.\n",
    "from ml_grid.util.synthetic_data_generator import generate_synthetic_data\n",
    "\n",
    "# --- 1. Setup Logging ---\n",
    "# This allows you to see the informative output from the generator and other steps.\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# --- 2. Generate Synthetic Data and the Ground-Truth Map ---\n",
    "logging.info(\"Generating a sample synthetic dataset using the importable function...\")\n",
    "synthetic_df, important_feature_map = generate_synthetic_data(\n",
    "    n_rows=200,\n",
    "    n_features=30,\n",
    "    n_outcome_vars=3,\n",
    "    feature_strength=0.7,\n",
    "    percent_important_features=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generated DataFrame Info (Before Imputation) ---\")\n",
    "print(f\"Shape: {synthetic_df.shape}\")\n",
    "print(f\"Total NaNs present: {synthetic_df.isnull().sum().sum()}\")\n",
    "print(\"Sample of data with missing values:\")\n",
    "display(synthetic_df.head())\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- 3. Calculate and Save the Percentage of Missing Values ---\n",
    "missing_pickle_filename = \"percent_missing_synthetic_data_generated.pkl\"\n",
    "print(f\"\\nCalculating missing value percentages and saving to '{missing_pickle_filename}'...\")\n",
    "save_missing_percentage(synthetic_df, output_file=missing_pickle_filename)\n",
    "print(\"✅ Missing value pickle file saved.\")\n",
    "\n",
    "\n",
    "# --- 4. Perform Mean Imputation ---\n",
    "print(\"\\nPerforming mean imputation on the dataset...\")\n",
    "# Get the list of outcome columns to exclude them from imputation\n",
    "outcome_columns = list(important_feature_map.keys())\n",
    "imputed_df = mean_impute_dataframe(data=synthetic_df.copy(), y_vars=outcome_columns)\n",
    "print(f\"Imputation complete. NaNs present after imputation: {imputed_df.isnull().sum().sum()}\")\n",
    "print(\"✅ Mean imputation successful.\")\n",
    "\n",
    "\n",
    "# --- 5. Save the Imputed Data to the Final CSV File ---\n",
    "output_csv_filename = \"synthetic_data_generated.csv\"\n",
    "imputed_df.to_csv(output_csv_filename, index=False)\n",
    "print(f\"\\nImputed data saved to '{output_csv_filename}'\")\n",
    "print(\"✅ Final CSV file saved.\")\n",
    "\n",
    "print(\"\\n--- Final Imputed DataFrame ---\")\n",
    "display(imputed_df.head())\n",
    "print(\"-------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Global Configuration\n",
    "\n",
    "Load `config_hyperopt.yml` once at the beginning to ensure all subsequent cells can access global settings and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "CONFIG_HYPEROPT_PATH = Path(\"../config_hyperopt.yml\")\n",
    "\n",
    "if not CONFIG_HYPEROPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Hyperopt configuration file not found at {CONFIG_HYPEROPT_PATH.resolve()}\")\n",
    "\n",
    "with open(CONFIG_HYPEROPT_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f) # Use 'config' as the main variable for the search\n",
    "print(\"  Hyperopt configuration loaded successfully.\")\n",
    "\n",
    "# Define project root (assuming notebook is run from project root)\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root set to: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal standalone script to instantiate and test the ml_grid.pipeline.data.pipe class.\n",
    "\n",
    "This script provides a clear example of the minimum setup required to create\n",
    "an `ml_grid_object`. It is intended for debugging the data pipeline in isolation.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "\n",
    "# --- Essential imports from your ml_grid project ---\n",
    "# Ensure your project is installed (e.g., `pip install -e .`) or the path is configured\n",
    "# so these imports can be found.\n",
    "try:\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components.\")\n",
    "    print(\"Please ensure you are running this script from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"1. Loading configuration from config.yml...\")\n",
    "CONFIG_PATH = project_root / \"../config_single_run.yml\"\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Configuration file not found at {CONFIG_PATH.resolve()}\")\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config_single_run = yaml.safe_load(f)\n",
    "print(\"  Configuration loaded successfully.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. SETUP ENVIRONMENT & PATHS FROM CONFIG\n",
    "# =============================================================================\n",
    "print(\"\\n2. Setting up environment and paths...\")\n",
    "\n",
    "# Project root is the current working directory\n",
    "# project_root is already defined from config_hyperopt.yml loading\n",
    "base_project_dir = str(project_root)\n",
    "print(f\"  Project Root: {project_root}\")\n",
    "\n",
    "# Load paths from config\n",
    "input_csv_path = project_root / config_single_run['data']['file_path']\n",
    "if not input_csv_path.exists():\n",
    "    print(f\"  ERROR: Data file not found at '{input_csv_path}'\")\n",
    "    print(\"  Please make sure the path is correct.\")\n",
    "    exit()\n",
    "print(f\"  Input CSV: {input_csv_path.resolve()}\")\n",
    "\n",
    "# Create experiment directory from config\n",
    "experiments_base_dir = project_root / config_single_run['experiment']['experiments_base_dir']\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=experiments_base_dir,\n",
    "    additional_naming=config_single_run['experiment']['additional_naming']\n",
    ")\n",
    "experiment_dir = Path(experiment_dir) # Ensure it's a Path object\n",
    "print(f\"  Experiment Directory: {experiment_dir}\")\n",
    "\n",
    "# Configure global parameters from config\n",
    "global_parameters.verbose = config_single_run['global_params']['verbose']\n",
    "global_parameters.error_raise = config_single_run['global_params']['error_raise']\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DEFINE PIPELINE PARAMETERS FROM CONFIG\n",
    "# =============================================================================\n",
    "print(\"\\n3. Defining pipeline parameters...\")\n",
    "\n",
    "drop_term_list = config_single_run['data']['drop_term_list']\n",
    "print(f\"  Drop terms: {drop_term_list}\")\n",
    "\n",
    "model_class_dict = config_single_run['models']\n",
    "print(f\"  Enabled models: {[k for k, v in model_class_dict.items() if v]}\")\n",
    "\n",
    "outcome_var = config_single_run['data'].get('outcome_var_override') # Use .get() for safety\n",
    "print(f\"  Outcome variable override: '{outcome_var}'\")\n",
    "\n",
    "local_param_dict = config_single_run['run_params']\n",
    "print(\"  Local parameter dictionary configured.\")\n",
    "\n",
    "# A unique index for this parameter combination (useful when iterating)\n",
    "param_space_index = 0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. INSTANTIATE THE PIPE CLASS\n",
    "# =============================================================================\n",
    "print(\"\\n4. Instantiating the 'pipe' class...\")\n",
    "\n",
    "try:\n",
    "    # This is the call to create the ml_grid_object.\n",
    "    # The entire data pipeline runs during this initialization.\n",
    "    ml_grid_object = pipe(\n",
    "        file_name=str(input_csv_path.resolve()),\n",
    "        drop_term_list=drop_term_list,\n",
    "        local_param_dict=local_param_dict,\n",
    "        base_project_dir=base_project_dir,\n",
    "        experiment_dir=experiment_dir,\n",
    "        test_sample_n=0,  # Use 0 to process the full dataset\n",
    "        param_space_index=param_space_index,\n",
    "        model_class_dict=model_class_dict,\n",
    "        outcome_var_override=outcome_var\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS: 'ml_grid_object' created successfully.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 5. INSPECT THE RESULTS\n",
    "    # =========================================================================\n",
    "    print(\"\\n5. Inspecting the final object attributes:\")\n",
    "    print(f\"  - Outcome Variable Used: {ml_grid_object.outcome_variable}\")\n",
    "    print(f\"  - X_train shape: {ml_grid_object.X_train.shape}\")\n",
    "    print(f\"  - y_train shape: {ml_grid_object.y_train.shape}\")\n",
    "    print(f\"  - X_test shape: {ml_grid_object.X_test.shape}\")\n",
    "    print(f\"  - y_test shape: {ml_grid_object.y_test.shape}\")\n",
    "    print(f\"  - Number of final features: {len(ml_grid_object.final_column_list)}\")\n",
    "    print(f\"  - Number of available models: {len(ml_grid_object.model_class_list)}\")\n",
    "    print(\"\\nFinal X_train columns sample:\")\n",
    "    print(ml_grid_object.X_train.columns.to_list()[:10])\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ERROR: Failed to instantiate the 'pipe' class.\")\n",
    "    print(\"=\"*50)\n",
    "    import traceback\n",
    "    print(f\"\\nAn error of type '{type(e).__name__}' occurred: {e}\")\n",
    "    print(\"\\nFull Traceback:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "print(\"\\n4. Initializing and executing the model training run...\")\n",
    "\n",
    "# --- Essential imports ---\n",
    "# Make sure your project is installed or the path is configured.\n",
    "try:\n",
    "    from ml_grid.pipeline.main import run\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components for the 'run' step.\")\n",
    "    print(\"Please ensure you are running this from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Use exit() if running as a script, or just let the error show in a notebook.\n",
    "    # exit()\n",
    "\n",
    "# Check if ml_grid_object exists from the previous cell's execution\n",
    "if 'ml_grid_object' not in locals() or 'local_param_dict' not in locals():\n",
    "    print(\"\\nERROR: 'ml_grid_object' or 'local_param_dict' not found.\")\n",
    "    print(\"Please ensure you have successfully run the previous cell (the 'pipe' instantiation script) first.\")\n",
    "else:\n",
    "    # REMOVED the top-level try...except block. Any exception will now halt the cell.\n",
    "\n",
    "    print(\"  Instantiating the 'run' class with the data object...\")\n",
    "    # Instantiate the 'run' class with the object from the data pipeline\n",
    "    run_instance = run(\n",
    "                local_param_dict=local_param_dict,\n",
    "                ml_grid_object=ml_grid_object,\n",
    "                global_params=config['global_params']\n",
    "            )\n",
    "\n",
    "    print(\"  Executing the hyperparameter search and cross-validation...\")\n",
    "    # Execute the hyperparameter search and cross-validation for all models.\n",
    "    # If an error occurs here, it will stop the notebook execution.\n",
    "    model_errors, highest_score = run_instance.execute()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS: Model training and evaluation complete.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\n5. Inspecting the training results:\")\n",
    "    print(f\"  - Highest score achieved across all models: {highest_score:.4f}\")\n",
    "\n",
    "    if model_errors:\n",
    "        print(f\"\\n  - {len(model_errors)} model(s) encountered errors during training:\")\n",
    "        for i, error_info in enumerate(model_errors):\n",
    "            try:\n",
    "                # Try to get a meaningful name for the model\n",
    "                model_name = error_info[0].__class__.__name__\n",
    "            except:\n",
    "                model_name = \"Unknown Model\"\n",
    "            error_exception = error_info[1]\n",
    "            print(f\"    {i+1}. Model: {model_name}, Error: {error_exception}\")\n",
    "    else:\n",
    "        print(\"\\n  - All configured models ran without critical errors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up test results directory\n",
    "!rm -r 'experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import yaml\n",
    "from hyperopt import hp\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD HYPEROPT CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"Loading configuration for Hyperopt search from config_hyperopt.yml...\")\n",
    "CONFIG_HYPEROPT_PATH = project_root / \"../config_hyperopt.yml\"\n",
    "\n",
    "if not CONFIG_HYPEROPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Hyperopt configuration file not found at {CONFIG_HYPEROPT_PATH.resolve()}\")\n",
    "\n",
    "with open(CONFIG_HYPEROPT_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f) # Use 'config' as the main variable for the search\n",
    "print(\"  Hyperopt configuration loaded successfully.\")\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD HYPEROPT SEARCH SPACE FROM CONFIG\n",
    "# =============================================================================\n",
    "def build_hyperopt_space(config_space):\n",
    "    \"\"\"Dynamically builds a hyperopt search space from the config dictionary.\"\"\"\n",
    "    space = {}\n",
    "    # The 'data' key is a nested dictionary of choices\n",
    "    space['data'] = {k: hp.choice(k, v) for k, v in config_space.get('data', {}).items()}\n",
    "\n",
    "    # All other keys are simple choices, excluding the settings key\n",
    "    for key, value in config_space.items():\n",
    "        # The key 'corr' in old notebooks is now 'correlation_threshold'\n",
    "        # This ensures backward compatibility if an old space is being tested.\n",
    "        if key == 'corr':\n",
    "            space['correlation_threshold'] = hp.choice('correlation_threshold', value)\n",
    "        elif key not in ['data', 'hyperopt_settings']:\n",
    "            space[key] = hp.choice(key, value)\n",
    "\n",
    "    return space\n",
    "\n",
    "# Build the hyperopt space object\n",
    "space = build_hyperopt_space(config['hyperopt_search_space'])\n",
    "\n",
    "print(\"\\nHyperopt search space built from config:\")\n",
    "print(space)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD OTHER PARAMETERS FROM CONFIG\n",
    "# =============================================================================\n",
    "model_class_dict = config['models']\n",
    "print(\"\\nModels to be used in Hyperopt search:\")\n",
    "print([model for model, enabled in model_class_dict.items() if enabled])\n",
    "\n",
    "# Define project root and construct absolute paths\n",
    "# project_root is already defined from config_hyperopt.yml loading\n",
    "base_project_dir = project_root / config['experiment']['experiments_base_dir']\n",
    "experiment_name = config['experiment']['additional_naming']\n",
    "multiple_outcomes_example = config['data']['multiple_outcomes']\n",
    "input_csv_path = project_root / config['data']['file_path']\n",
    "drop_term_list = config['data']['drop_term_list']\n",
    "max_evals = config['hyperopt_settings']['max_evals']\n",
    "\n",
    "print(\"\\nExperiment settings:\")\n",
    "print(f\"  Base Directory: {base_project_dir.resolve()}\")\n",
    "print(f\"  Experiment Name: {experiment_name}\")\n",
    "print(f\"  Input CSV: {input_csv_path.resolve()}\")\n",
    "print(f\"  Multiple Outcomes: {multiple_outcomes_example}\")\n",
    "print(f\"  Max Evals per Outcome: {max_evals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import (\n",
    "    ParameterGrid,\n",
    "    RepeatedKFold,\n",
    "    cross_validate,\n",
    ")\n",
    "\n",
    "# --- Essential imports from your ml_grid project ---\n",
    "# Ensure your project is installed or the path is configured.\n",
    "try:\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.pipeline.hyperparameter_search import HyperparameterSearch\n",
    "    from ml_grid.util.bayes_utils import calculate_combinations\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "    from ml_grid.util.project_score_save import project_score_save_class\n",
    "    from ml_grid.util.validate_parameters import validate_parameters_helper\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components.\")\n",
    "    print(\"Please ensure you are running this from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Use exit() if running as a script, or just let the error show in a notebook.\n",
    "    # exit()\n",
    "\n",
    "# =============================================================================\n",
    "# STANDALONE SCRIPT TO DEBUG `grid_search_crossvalidate` INTERNALS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"   Project Root: {base_project_dir}\")\n",
    "\n",
    "# --- Define the path to your input data ---\n",
    "# IMPORTANT: Update this path to the correct location of your file.\n",
    "# Now relative to the project_root\n",
    "input_csv_path = project_root / \"synthetic_data_generated.csv\"\n",
    "\n",
    "if not input_csv_path.exists():\n",
    "    print(f\"   ERROR: Data file not found at '{input_csv_path.resolve()}'\")\n",
    "    print(\"   Please make sure the path is correct.\")\n",
    "    # exit()\n",
    "else:\n",
    "    print(f\"   Input CSV: {input_csv_path.resolve()}\")\n",
    "\n",
    "# Create a directory for this specific experiment run's logs and outputs\n",
    "experiments_base_dir = project_root / config['experiment']['experiments_base_dir']\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=experiments_base_dir,\n",
    "    additional_naming=\"GSCV_Internals_Debug\"\n",
    ")\n",
    "experiment_dir = Path(experiment_dir) # Ensure it's a Path object\n",
    "print(f\"   Experiment Directory: {experiment_dir.resolve()}\")\n",
    "\n",
    "\n",
    "# --- 2. Configure Parameters ---\n",
    "print(\"\\n2. Configuring parameters...\")\n",
    "# Global parameters\n",
    "global_parameters.verbose = 1\n",
    "global_parameters.error_raise = True\n",
    "global_parameters.bayessearch = False\n",
    "global_parameters.random_grid_search = True\n",
    "global_parameters.sub_sample_param_space_pct = 0.2\n",
    "\n",
    "# Local parameters for the data pipeline, configured for your dataset\n",
    "local_param_dict = {\n",
    "    'outcome_var_n': 1,\n",
    "    'param_space_size': 'xsmall',\n",
    "    'scale': True,\n",
    "    'feature_n': 90,\n",
    "    'use_embedding': False,\n",
    "    'percent_missing': 98,\n",
    "    'correlation_threshold': 0.95,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'data': {\n",
    "        'age': True, 'sex': True, 'bmi': True, 'ethnicity': True,\n",
    "        'bloods': True, 'diagnostic_order': True, 'drug_order': True,\n",
    "        'annotation_n': True, 'meta_sp_annotation_n': True,\n",
    "        'annotation_mrc_n': True, 'meta_sp_annotation_mrc_n': True,\n",
    "        'core_02': True, 'bed': True, 'vte_status': True,\n",
    "        'hosp_site': True, 'core_resus': True, 'news': True,\n",
    "        'date_time_stamp': False, 'appointments': False,\n",
    "    }\n",
    "}\n",
    "print(\"   Parameters configured.\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "ml_grid_object = None\n",
    "try:\n",
    "    # --- 3. Run Data Pipeline to Get `ml_grid_object` ---\n",
    "    print(\"\\n3. Initializing data pipeline (`pipe`) to prepare data...\")\n",
    "    ml_grid_object = pipe(\n",
    "        file_name=str(input_csv_path),\n",
    "        drop_term_list=config['data']['drop_term_list'],\n",
    "        experiment_dir=str(experiment_dir),\n",
    "        base_project_dir=base_project_dir,\n",
    "        local_param_dict=local_param_dict,\n",
    "        param_space_index=0,\n",
    "        model_class_dict=config['models'],\n",
    "        outcome_var_override='outcome_var_1'\n",
    "    )\n",
    "    print(\"   Data pipeline finished.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- 4. EXPLICIT `grid_search_crossvalidate` INTERNAL LOGIC ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n4. Executing `grid_search_crossvalidate` internal logic...\")\n",
    "    start_time_main = time.time()\n",
    "\n",
    "    # --- 4a. Select a model and extract its properties ---\n",
    "    model_to_test = ml_grid_object.model_class_list[0]\n",
    "    algorithm_implementation = model_to_test.algorithm_implementation\n",
    "    parameter_space = model_to_test.parameter_space\n",
    "    method_name = model_to_test.method_name\n",
    "    print(f\"   - Model for debugging: {method_name}\")\n",
    "\n",
    "    # --- 4b. Initialize variables from `grid_search_crossvalidate.__init__` ---\n",
    "    # CORRECTED: Set each warning filter individually.\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "    grid_n_jobs = global_parameters.grid_n_jobs\n",
    "    if \"keras\" in method_name.lower() or \"xgb\" in method_name.lower() or \"catboost\" in method_name.lower():\n",
    "        grid_n_jobs = 1\n",
    "        try:\n",
    "            gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except Exception as e:\n",
    "            print(f\"   - Could not configure GPU for TensorFlow: {e}\")\n",
    "\n",
    "    # Extract data from the ml_grid_object\n",
    "    X_train, y_train = ml_grid_object.X_train, ml_grid_object.y_train\n",
    "    X_test, y_test = ml_grid_object.X_test, ml_grid_object.y_test\n",
    "\n",
    "    # --- 4c. Prepare for Hyperparameter Search ---\n",
    "    max_param_space_iter_value = global_parameters.max_param_space_iter_value\n",
    "    param_grid_size = len(ParameterGrid(parameter_space))\n",
    "    sub_sample_parameter_val = int(global_parameters.sub_sample_param_space_pct * param_grid_size)\n",
    "    n_iter_v = max(2, sub_sample_parameter_val)\n",
    "    n_iter_v = min(n_iter_v, max_param_space_iter_value)\n",
    "    print(f\"   - Hyperparameter search iterations (n_iter): {n_iter_v}\")\n",
    "\n",
    "    # Instantiate the HyperparameterSearch class\n",
    "    search = HyperparameterSearch(\n",
    "        algorithm=algorithm_implementation,\n",
    "        parameter_space=parameter_space,\n",
    "        method_name=method_name,\n",
    "        global_params=global_parameters,\n",
    "        max_iter=n_iter_v,\n",
    "        ml_grid_object=ml_grid_object\n",
    "    )\n",
    "\n",
    "    # --- 4d. Run the Hyperparameter Search ---\n",
    "    print(\"   - Running HyperparameterSearch.run_search()...\")\n",
    "    # This is the core search step (e.g., RandomizedSearchCV.fit())\n",
    "    best_estimator = search.run_search(X_train, y_train)\n",
    "    print(f\"   - Best estimator found: {best_estimator.get_params()}\")\n",
    "\n",
    "    # --- 4e. Fit the Final Model and Evaluate ---\n",
    "    print(\"   - Fitting the best estimator on the full training data...\")\n",
    "    # Use numpy arrays for final fitting\n",
    "    best_estimator.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # --- 4f. Run Cross-Validation on the Best Model ---\n",
    "    print(\"   - Running cross_validate on the best estimator...\")\n",
    "    cv_splitter = RepeatedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
    "\n",
    "    try:\n",
    "        scores = cross_validate(\n",
    "            best_estimator,\n",
    "            X_train.values,\n",
    "            y_train.values,\n",
    "            scoring=global_parameters.metric_list,\n",
    "            cv=cv_splitter,\n",
    "            n_jobs=grid_n_jobs,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        failed = False\n",
    "        print(\"   - Cross-validation successful.\")\n",
    "        for metric, values in scores.items():\n",
    "            print(f\"     - {metric}: {np.mean(values):.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   - Cross-validation failed: {e}\")\n",
    "        scores = {metric: [0.5] for metric in global_parameters.metric_list} # Default scores\n",
    "        failed = True\n",
    "\n",
    "    # --- 4g. Predict on the Test Set and Calculate Final Score ---\n",
    "    print(\"   - Predicting on the test set...\")\n",
    "    # Use .values to ensure numpy array for prediction\n",
    "    best_pred_orig = best_estimator.predict(X_test.values)\n",
    "\n",
    "    # The final score to be optimized/reported\n",
    "    final_auc_score = roc_auc_score(y_test, best_pred_orig)\n",
    "\n",
    "    score_saver = project_score_save_class(experiment_dir=experiment_dir)\n",
    "\n",
    "\n",
    "    # --- 4h. Log the results (emulating project_score_save_class) ---\n",
    "    score_saver.update_score_log(\n",
    "        ml_grid_object=ml_grid_object,\n",
    "        scores=scores,\n",
    "        best_pred_orig=best_pred_orig,\n",
    "        current_algorithm=best_estimator,\n",
    "        method_name=method_name,\n",
    "        pg=param_grid_size,\n",
    "        start=start_time_main,\n",
    "        n_iter_v=n_iter_v,\n",
    "        failed=failed\n",
    "    )\n",
    "    print(\"   - Results logged.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- 5. Display the Final Results ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS: Standalone internal logic run complete.\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n   - Model Tested: {method_name}\")\n",
    "    print(f\"   - Final Reported AUC Score on Test Set: {final_auc_score:.4f}\")\n",
    "    print(f\"   - Total execution time: {time.time() - start_time_main:.2f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"A CRITICAL ERROR OCCURRED DURING EXECUTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Error Type: {type(e).__name__}\")\n",
    "    print(f\"Error Message: {e}\")\n",
    "    print(\"\\nFull Traceback:\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    # --- 6. Cleanup ---\n",
    "    try:\n",
    "        os.remove('final_grid_score_log.csv')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        pass # File might not exist if no runs completed\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup of final_grid_score_log.csv: {e}\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(experiments_base_dir)\n",
    "    except FileNotFoundError:\n",
    "        pass # Directory might not exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup of experiment directory: {e}\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree('run_0')\n",
    "    except FileNotFoundError:\n",
    "        pass # Directory might not exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup of run_0: {e}\")\n",
    "    print(\"\\n6. Clean up complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "# print all attributes and their values\n",
    "print(vars(global_parameters))\n",
    "\n",
    "if global_parameters.debug_level > 1:\n",
    "        print(\"Debug Mode: Additional logging enabled.\")\n",
    "\n",
    "# Update global parameters\n",
    "#global_parameters.update_parameters(debug_level=0, grid_n_jobs = -1, error_raise = True, max_param_space_iter_value=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "#print all attributes and their values\n",
    "\n",
    "print(vars(global_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "from hyperopt import STATUS_FAIL\n",
    "\n",
    "from ml_grid.pipeline.data import NoFeaturesError, pipe\n",
    "from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# --- Setup Experiment Directory and Logging ---\n",
    "# This is the main directory for the entire Hyperopt search.\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=base_project_dir, # From cell 7\n",
    "    additional_naming=experiment_name # From cell 7\n",
    ")\n",
    "experiment_dir = Path(experiment_dir)\n",
    "print(f\"Main experiment directory: {experiment_dir.resolve()}\")\n",
    "\n",
    "# Initialize the project-level score log within the main experiment directory\n",
    "project_score_save_class(experiment_dir)\n",
    "\n",
    "def objective(local_param_dict, outcome_var=None):\n",
    "    \"\"\"The objective function that Hyperopt will minimize.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Evaluating for outcome: {outcome_var}\")\n",
    "    print(f\"Params: {local_param_dict}\")\n",
    "\n",
    "    # A unique ID for this specific trial\n",
    "    trial_idx = random.randint(0, 9999999999)\n",
    "\n",
    "    try:\n",
    "        ml_grid_object = pipe(\n",
    "            file_name=str(input_csv_path.resolve()),\n",
    "            drop_term_list=drop_term_list,\n",
    "            local_param_dict=local_param_dict,\n",
    "            base_project_dir=base_project_dir,\n",
    "            experiment_dir=experiment_dir,\n",
    "            test_sample_n=0,\n",
    "            param_space_index=trial_idx,\n",
    "            model_class_dict=model_class_dict,\n",
    "            outcome_var_override=outcome_var\n",
    "        )\n",
    "\n",
    "        from ml_grid.pipeline import main\n",
    "\n",
    "        # Run the modeling pipeline and get the best score for this trial\n",
    "        errors, highest_score = main.run(ml_grid_object, local_param_dict=local_param_dict).execute()\n",
    "\n",
    "        return {\n",
    "            'loss': 1 - float(highest_score), # Hyperopt minimizes, so we use 1 - AUC\n",
    "            'status': 'ok'\n",
    "        }\n",
    "    except NoFeaturesError as e:\n",
    "        print(f\"Skipping trial due to NoFeaturesError: {e}\")\n",
    "        return {'status': STATUS_FAIL, 'loss': float('inf')}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        return {'status': STATUS_FAIL, 'loss': float('inf')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import STATUS_OK, Trials, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun\n",
    "if( multiple_outcomes_example == False):\n",
    "\n",
    "    # Fix the additional argument (outcome_var) using partial\n",
    "    # Define the single outcome to test\n",
    "    single_outcome_var = 'outcome_var_1'\n",
    "    objective_with_outcome = partial(objective, outcome_var=single_outcome_var)\n",
    "\n",
    "    # Initialize Trials object to store results\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the optimization\n",
    "    best = fmin(\n",
    "        fn=objective_with_outcome,  # Use the partial function\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    # Assuming the log file is in the current experiment_dir\n",
    "    results_df = pd.read_csv(experiment_dir / 'final_grid_score_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    if 'results_df' in locals():\n",
    "        display(results_df.sort_values('auc', ascending=False).iloc[0])\n",
    "    else:\n",
    "        print(\"results_df not found. Please run the previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    if 'results_df' in locals():\n",
    "        display(results_df.sort_values('auc', ascending=False))\n",
    "    else:\n",
    "        print(\"results_df not found. Please run the previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == True):\n",
    "\n",
    "    dft = pd.read_csv(input_csv_path.resolve(), nrows=1)\n",
    "    dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outcome variables by finding prefix \"outcome_var_\" in column list\n",
    "\n",
    "if( multiple_outcomes_example == True):\n",
    "    outcome_var_list = [dft.columns[i] for i in range(len(dft.columns)) if \"outcome_var_\" in dft.columns[i]]\n",
    "\n",
    "    outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun -s cumulative -l 50\n",
    "#%%prun\n",
    "if multiple_outcomes_example:\n",
    "    import multiprocessing\n",
    "    import sys\n",
    "    import traceback\n",
    "    from datetime import datetime\n",
    "    from functools import partial\n",
    "    from pathlib import Path\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # Assuming config_parser is in notebooks/ or a discoverable path\n",
    "    from config_parser import load_config\n",
    "    from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "    from joblib import Parallel, delayed\n",
    "\n",
    "    # --- Imports from your ml_grid project ---\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    # Import time_limit to handle timeouts\n",
    "    from ml_grid.pipeline.main import run, time_limit\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "\n",
    "    # ========== TOGGLE FOR SINGLE-THREADED MODE ==========\n",
    "    USE_SINGLE_THREADED = True  # Set to True for single-threaded execution\n",
    "    # =====================================================\n",
    "\n",
    "    # 1. --- Load Configuration from YAML ---\n",
    "    config = load_config('config_hyperopt.yml')\n",
    "\n",
    "    # 2. --- Set up Experiment Environment ---\n",
    "    project_root = Path.cwd() \n",
    "    experiments_base_dir = project_root / config['experiment']['experiments_base_dir']\n",
    "    experiment_dir = create_experiment_directory(\n",
    "        base_dir=experiments_base_dir,\n",
    "        additional_naming=config['experiment']['additional_naming']\n",
    "    )\n",
    "\n",
    "    # 3. --- Dynamically Build Hyperopt Search Space from Config ---\n",
    "    space = {}\n",
    "    for key, value in config['hyperopt_search_space'].items():\n",
    "        if key == 'data':\n",
    "            space['data'] = {k: hp.choice(f'data_{k}', v) for k, v in value.items()}\n",
    "        else:\n",
    "            space[key] = hp.choice(key, value)\n",
    "\n",
    "    # 4. --- Get Hyperopt Settings from Config ---\n",
    "    max_evals = config['hyperopt_settings']['max_evals']\n",
    "    # Retrieve trial timeout, defaulting to None if not set\n",
    "    trial_timeout = config['hyperopt_settings'].get('trial_timeout', None)\n",
    "\n",
    "    # 5. --- Determine Outcome Variables & PRE-LOAD DATA ---\n",
    "    data_file_path = config['data']['file_path']\n",
    "    if not Path(data_file_path).is_absolute():\n",
    "        data_file_path = project_root / data_file_path\n",
    "\n",
    "    # OPTIMIZATION: Load data ONCE here\n",
    "    print(f\"Pre-loading data from {data_file_path}...\", flush=True)\n",
    "    GLOBAL_DF = pd.read_csv(data_file_path)\n",
    "\n",
    "    outcome_var_list = []\n",
    "    if config['data'].get('multiple_outcomes', False):\n",
    "        outcome_var_list = [col for col in GLOBAL_DF.columns if 'outcome_var_' in col]\n",
    "\n",
    "        if not outcome_var_list:\n",
    "            raise ValueError(f\"No outcome variables found with 'outcome_var_' prefix in {data_file_path}\")\n",
    "        print(f\"Found {len(outcome_var_list)} outcome variables to process.\")\n",
    "    else:\n",
    "        outcome_var_list = config['hyperopt_search_space']['outcome_var_n']\n",
    "\n",
    "    # 6. --- Define the Objective Function for Hyperopt ---\n",
    "    def objective(params, outcome_var, loaded_df):\n",
    "        \"\"\"\n",
    "        Objective function for hyperopt. It receives sampled parameters,\n",
    "        the specific outcome variable, and the PRE-LOADED DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Wrap the entire trial execution in the time_limit context manager\n",
    "            with time_limit(trial_timeout):\n",
    "                local_param_dict = params\n",
    "\n",
    "                # Initialize the data pipeline using the cached DataFrame\n",
    "                ml_grid_object = pipe(\n",
    "                    file_name=None, # Not needed when input_df is provided\n",
    "                    drop_term_list=config['data']['drop_term_list'],\n",
    "                    model_class_dict=config['models'],\n",
    "                    local_param_dict=local_param_dict,\n",
    "                    base_project_dir=project_root,\n",
    "                    experiment_dir=experiment_dir,\n",
    "                    param_space_index=0, \n",
    "                    outcome_var_override=outcome_var,\n",
    "                    input_df=loaded_df # <--- PASS CACHED DATA\n",
    "                )\n",
    "\n",
    "                # Execute the models\n",
    "                run_instance = run(local_param_dict=local_param_dict, ml_grid_object=ml_grid_object)\n",
    "                _, highest_score = run_instance.execute()\n",
    "\n",
    "                return {'loss': -highest_score, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "        except TimeoutError:\n",
    "            # Handle the specific trial timeout\n",
    "            print(f\"TIMEOUT in objective for {outcome_var} (Limit: {trial_timeout}s)\", file=sys.stderr)\n",
    "            return {'loss': float('inf'), 'status': 'fail', 'message': 'Trial Timeout'}\n",
    "\n",
    "        except Exception as e:\n",
    "            tb_str = traceback.format_exc()\n",
    "            print(f\"ERROR in objective for {outcome_var} with params {params}: {e}\\n{tb_str}\", file=sys.stderr)\n",
    "            return {'loss': float('inf'), 'status': 'fail', 'message': str(e)}\n",
    "\n",
    "    # 7. --- Define the Worker Function for Parallel Processing ---\n",
    "    def process_single_outcome(outcome_var):\n",
    "        start_time = datetime.now()\n",
    "        print(f\"[{start_time}] Starting optimization for outcome: {outcome_var}\", flush=True)\n",
    "\n",
    "        # Pass the global dataframe to the objective function via partial\n",
    "        # Joblib will handle the serialization/shared memory of GLOBAL_DF efficiently\n",
    "        fmin_objective = partial(objective, outcome_var=outcome_var, loaded_df=GLOBAL_DF)\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(\n",
    "            fn=fmin_objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=trials,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        end_time = datetime.now()\n",
    "        failed_trials = [t for t in trials.results if t['status'] == 'fail']\n",
    "\n",
    "        print(f\"[{end_time}] Finished {outcome_var} (Duration: {end_time - start_time})\", flush=True)\n",
    "        print(f\"  -&gt; Best param set for this outcome: {best}\", flush=True)\n",
    "        print(f\"  -&gt; Trials summary: {len(failed_trials)}/{len(trials.results)} failed.\", flush=True)\n",
    "\n",
    "        return (outcome_var, best)\n",
    "\n",
    "    # 8. --- Main Execution Block ---\n",
    "    start_total = datetime.now()\n",
    "    \n",
    "    if USE_SINGLE_THREADED:\n",
    "        print(f\"Starting all optimizations at {start_total} using SINGLE-THREADED mode.\")\n",
    "        results = []\n",
    "        for outcome in outcome_var_list:\n",
    "            result = process_single_outcome(outcome)\n",
    "            results.append(result)\n",
    "    else:\n",
    "        num_cores = max(1, multiprocessing.cpu_count() - 2)\n",
    "        print(f\"Starting all optimizations at {start_total} using {num_cores} cores.\")\n",
    "        results = Parallel(n_jobs=num_cores, verbose=10)(\n",
    "            delayed(process_single_outcome)(outcome)\n",
    "            for outcome in outcome_var_list\n",
    "        )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Hyperparameter Optimization Summary\")\n",
    "    print(\"=\"*70)\n",
    "    for outcome_var, best_params in results:\n",
    "        print(f\"\\n✅ Success for '{outcome_var}':\")\n",
    "        print(f\"   Best parameter combination found: {best_params}\")\n",
    "\n",
    "    end_total = datetime.now()\n",
    "    print(f\"\\nCompleted all optimizations at {end_total} (Total duration: {end_total - start_total})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the parent directory from config\n",
    "parent_dir = experiment_dir # Use the path from the run\n",
    "\n",
    "# Check if the CSV is directly in the parent directory first\n",
    "csv_path = os.path.join(parent_dir, 'final_grid_score_log.csv')\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    # If not found directly, look for it in timestamped subfolders\n",
    "    # List all folders in the parent directory that match the date pattern\n",
    "    folders = [f for f in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, f))]\n",
    "\n",
    "    # Parse folder names as dates and find the latest one\n",
    "    def parse_date(folder_name: str):\n",
    "        \"\"\"\n",
    "        Parses the timestamp from the beginning of a folder name.\n",
    "        Expected format: 'YYYY-MM-DD_HH-MM-SS_...'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # The timestamp is always the first 19 characters.\n",
    "            timestamp_part = folder_name[:19]\n",
    "            return datetime.strptime(timestamp_part, '%Y-%m-%d_%H-%M-%S')\n",
    "        except (ValueError, IndexError):\n",
    "            # Return None if the folder name doesn't match the expected format or is too short.\n",
    "            return None\n",
    "\n",
    "    # Filter and sort folders by date\n",
    "    folders_with_dates = [(f, parse_date(f)) for f in folders]\n",
    "    folders_with_dates = [f for f in folders_with_dates if f[1] is not None]\n",
    "\n",
    "    if folders_with_dates:\n",
    "        latest_folder = max(folders_with_dates, key=lambda x: x[1])[0]  # Get the folder with the latest date\n",
    "        print(\"latest_folder\", latest_folder)\n",
    "\n",
    "        # Construct the path to the CSV file in the latest folder\n",
    "        csv_path = os.path.join(parent_dir, latest_folder, 'final_grid_score_log.csv')\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No timestamped folders found and CSV not in parent directory\")\n",
    "else:\n",
    "    print(\"CSV found directly in parent directory\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sort the DataFrame by 'auc' column in descending order\n",
    "df = df.sort_values(by='auc', ascending=False)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Group by outcome_variable and display the first row of each group with the highest auc\n",
    "df_best = df.groupby('outcome_variable').apply(lambda x: x.iloc[0])\n",
    "\n",
    "# Display the result\n",
    "df_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best['algorithm_implementation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- Configuration ---\n",
    "experiments_base_dir = Path(config['experiment']['experiments_base_dir'])\n",
    "\n",
    "# --- Find the CSV file (try multiple locations) ---\n",
    "\n",
    "def find_csv_file():\n",
    "    \"\"\"\n",
    "    Search for final_grid_score_log.csv in multiple locations:\n",
    "    1. Project root (parent of experiments_base_dir)\n",
    "    2. Directly in experiments_base_dir\n",
    "    3. In the latest timestamped subfolder\n",
    "    \"\"\"\n",
    "    # Location 1: Project root (where the notebook is run from)\n",
    "    project_root = experiments_base_dir.parent\n",
    "    csv_path = project_root / 'final_grid_score_log.csv'\n",
    "    if csv_path.exists():\n",
    "        print(f\"✓ CSV found in project root: {csv_path.resolve()}\")\n",
    "        return csv_path\n",
    "\n",
    "    # Location 2: Directly in experiments directory\n",
    "    csv_path = project_root / experiments_base_dir / 'final_grid_score_log.csv'\n",
    "    if csv_path.exists():\n",
    "        print(f\"✓ CSV found in experiments directory: {csv_path.resolve()}\")\n",
    "        return csv_path\n",
    "\n",
    "    # Location 3: In latest timestamped subfolder\n",
    "    latest_folder = find_latest_experiment_folder()\n",
    "    if latest_folder:\n",
    "        csv_path = latest_folder / 'final_grid_score_log.csv' # latest_folder is already absolute\n",
    "        if csv_path.exists():\n",
    "            print(f\"✓ CSV found in latest experiment folder: {csv_path.resolve()}\")\n",
    "            return csv_path\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_date(folder_name: str):\n",
    "    \"\"\"\n",
    "    Parses the timestamp from the beginning of a folder name.\n",
    "    Expected format: 'YYYY-MM-DD_HH-MM-SS_...'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        timestamp_part = folder_name[:19]\n",
    "        return datetime.strptime(timestamp_part, '%Y-%m-%d_%H-%M-%S')\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_latest_experiment_folder():\n",
    "    \"\"\"Find the most recent timestamped experiment folder.\"\"\"\n",
    "    if not experiments_base_dir.exists() or not experiments_base_dir.is_dir():\n",
    "        print(f\"⚠ Experiments directory not found: {(project_root / experiments_base_dir).resolve()}\")\n",
    "        return None\n",
    "\n",
    "    subfolders = [f for f in experiments_base_dir.iterdir() if f.is_dir()]\n",
    "    folders_with_dates = [(f, parse_date(f.name)) for f in subfolders]\n",
    "    valid_folders = [f for f in folders_with_dates if f[1] is not None]\n",
    "\n",
    "    if valid_folders:\n",
    "        latest_folder = max(valid_folders, key=lambda x: x[1])[0]\n",
    "        print(f\"Latest experiment folder: {latest_folder.name}\")\n",
    "        return latest_folder\n",
    "    else:\n",
    "        print(\"⚠ No valid timestamped experiment folders found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Find the CSV file\n",
    "log_file_path = find_csv_file()\n",
    "\n",
    "if log_file_path and log_file_path.exists():\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(log_file_path)\n",
    "\n",
    "    # Sort by AUC in descending order\n",
    "    df_sorted = df.sort_values(by='auc', ascending=False)\n",
    "\n",
    "    print(f\"\\n✓ Successfully loaded {len(df_sorted)} records from the log file.\")\n",
    "\n",
    "    # Group by outcome_variable and get the best result for each\n",
    "    top_results_by_outcome = df_sorted.groupby('outcome_variable').first().reset_index()\n",
    "\n",
    "    print(f\"\\nTop results by outcome variable ({len(top_results_by_outcome)} outcomes):\\n\")\n",
    "\n",
    "    # Display the result\n",
    "    display(top_results_by_outcome)\n",
    "\n",
    "else:\n",
    "    print(\"\\n✗ Error: Could not find 'final_grid_score_log.csv' in any expected location:\")\n",
    "    print(f\"  - {(experiments_base_dir.parent / 'final_grid_score_log.csv').resolve()}\")\n",
    "    print(f\"  - {(project_root / experiments_base_dir / 'final_grid_score_log.csv').resolve()}\")\n",
    "    print(f\"  - In any timestamped subfolder within {experiments_base_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Shape of the dataset: {data.shape}\")\n",
    "print(f\"Columns: {data.columns.tolist()}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for constant features\n",
    "print(\"\\n=== Constant Features ===\")\n",
    "constant_features = [col for col in data.columns if data[col].nunique() == 1]\n",
    "print(f\"Constant features: {constant_features}\")\n",
    "\n",
    "# Check for features with very low variance (almost constant)\n",
    "print(\"\\n=== Low Variance Features ===\")\n",
    "low_variance_features = []\n",
    "for col in data.columns:\n",
    "    if data[col].dtype in [np.float64, np.int64]:  # Check only numeric features\n",
    "        if data[col].std() < 0.01:  # Threshold for low variance\n",
    "            low_variance_features.append(col)\n",
    "print(f\"Low variance features: {low_variance_features}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\n=== Duplicate Rows ===\")\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for class distribution (if it's a classification problem)\n",
    "if 'target' in data.columns:  # Replace 'target' with your actual target column name\n",
    "    print(\"\\n=== Class Distribution ===\")\n",
    "    print(data['target'].value_counts())\n",
    "\n",
    "# Check for categorical features with high cardinality\n",
    "print(\"\\n=== High Cardinality Categorical Features ===\")\n",
    "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
    "high_cardinality_features = [col for col in categorical_features if data[col].nunique() > 100]\n",
    "print(f\"High cardinality categorical features: {high_cardinality_features}\")\n",
    "\n",
    "# Check for outliers in numeric features (using IQR)\n",
    "print(\"\\n=== Outliers in Numeric Features ===\")\n",
    "numeric_features = data.select_dtypes(include=[np.float64, np.int64]).columns\n",
    "for col in numeric_features:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {col}: {len(outliers)} rows\")\n",
    "\n",
    "# Summary of issues\n",
    "print(\"\\n=== Summary of Issues ===\")\n",
    "if missing_values.any():\n",
    "    print(f\"- Missing values found in {missing_values[missing_values > 0].index.tolist()}\")\n",
    "if constant_features:\n",
    "    print(f\"- Constant features found: {constant_features}\")\n",
    "if low_variance_features:\n",
    "    print(f\"- Low variance features found: {low_variance_features}\")\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"- Duplicate rows found: {duplicate_rows}\")\n",
    "if high_cardinality_features:\n",
    "    print(f\"- High cardinality categorical features found: {high_cardinality_features}\")\n",
    "if not missing_values.any() and not constant_features and not low_variance_features and not duplicate_rows and not high_cardinality_features:\n",
    "    print(\"- No major issues found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "\n",
    "# Get the top result for each outcome_variable by AUC\n",
    "top_auc_per_outcome = df.loc[df.groupby('outcome_variable')['auc'].idxmax()]\n",
    "\n",
    "# Sort by AUC for better visualization\n",
    "top_auc_per_outcome = top_auc_per_outcome.sort_values(by='auc', ascending=False)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create barplot to show the top AUC for each outcome_variable\n",
    "sns.barplot(\n",
    "    x='auc',\n",
    "    y='outcome_variable',\n",
    "    data=top_auc_per_outcome,\n",
    "    hue='nb_size',\n",
    "    dodge=False,\n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top AUC for Each Outcome Variable')\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Outcome Variable')\n",
    "plt.legend(title='num features')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes\n",
    "import pandas as pd\n",
    "\n",
    "from ml_grid.results_processing.core import ResultsAggregator\n",
    "from ml_grid.results_processing.plot_master import MasterPlotter\n",
    "\n",
    "# 1. Load your data using the ResultsAggregator\n",
    "#    Replace with the actual path to your results and feature names file.\n",
    "#    The feature_names_csv is optional but required for feature-related plots.\n",
    "try:\n",
    "    aggregator = ResultsAggregator(\n",
    "        root_folder=config['experiment']['experiments_base_dir'],\n",
    "        feature_names_csv=config['data']['file_path'])\n",
    "    results_df = aggregator.aggregate_all_runs()\n",
    "\n",
    "    # 2. Instantiate the MasterPlotter with your data\n",
    "    master_plotter = MasterPlotter(results_df)\n",
    "\n",
    "    # 3. Call the plot_all() method to generate all visualizations\n",
    "    #    You can customize the primary metric and other options.\n",
    "    master_plotter.plot_all(metric='auc', stratify_by_outcome=True)\n",
    "\n",
    "except (ValueError, FileNotFoundError) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure your results folder path is correct and contains valid run data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['failed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='run_time', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_grid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
