{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import warnings\n",
    "import sys\n",
    "import pandas as pd\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.pyll import scope\n",
    "os.environ[\"PYTHONWARNINGS\"] = \"ignore::UserWarning\"\n",
    "warnings.filterwarnings('ignore') \n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag to control CPU core binding\n",
    "limit_cpu_cores = False\n",
    "\n",
    "if limit_cpu_cores:\n",
    "    # Get the current process ID\n",
    "    pid = os.getpid()\n",
    "    print(f\"Notebook PID: {pid}\")\n",
    "\n",
    "    # Define the CPU cores to bind (e.g., cores 0-3)\n",
    "    core_range = \"0-3\"\n",
    "\n",
    "    # Use taskset to bind the current process to specific CPU cores\n",
    "    try:\n",
    "        # Execute taskset command\n",
    "        subprocess.run([\"taskset\", \"-cp\", core_range, str(pid)], check=True)\n",
    "        print(f\"Successfully bound Notebook PID {pid} to CPU cores {core_range}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'taskset' command not found. Please ensure it is installed.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while setting CPU affinity: {e}\")\n",
    "else:\n",
    "    print(\"CPU core binding is disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util import grid_param_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as ipw\n",
    "output = ipw.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "            \n",
    "            'resample' : ['undersample', 'oversample', None],\n",
    "            'scale'    : [True, False],\n",
    "            'feature_n': [100, 95, 75, 50, 25, 5],\n",
    "            'param_space_size':['medium', 'xsmall'],\n",
    "            'n_unique_out': [10],\n",
    "            'outcome_var_n':['1'],\n",
    "                            'percent_missing':[99, 95, 80],  #n/100 ex 95 for 95% # 99.99, 99.5, 9\n",
    "                            'corr':[0.98, 0.85, 0.5, 0.25],\n",
    "                            'data':[{'age':[True, False],\n",
    "                                    'sex':[True, False],\n",
    "                                    'bmi':[True],\n",
    "                                    'ethnicity':[True, False],\n",
    "                                    'bloods':[True, False],\n",
    "                                    'diagnostic_order':[True, False],\n",
    "                                    'drug_order':[True, False],\n",
    "                                    'annotation_n':[True, False],\n",
    "                                    'meta_sp_annotation_n':[True, False],\n",
    "                                    'annotation_mrc_n':[True, False],\n",
    "                                    'meta_sp_annotation_mrc_n':[True, False],\n",
    "                                    'core_02':[False],\n",
    "                                    'bed':[False],\n",
    "                                    'vte_status':[True],\n",
    "                                    'hosp_site':[True],\n",
    "                                    'core_resus':[False],\n",
    "                                    'news':[False],\n",
    "                                    'date_time_stamp':[ False]\n",
    "                                    \n",
    "                                    }]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "space = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [100, 95, 75, 50, 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']),\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'feature_selection_method': hp.choice('feature_selection_method', ['anova', 'markov_blanket']),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [True, False]),\n",
    "        'sex': hp.choice('sex', [True, False]),\n",
    "        'bmi': hp.choice('bmi', [True]),\n",
    "        'ethnicity': hp.choice('ethnicity', [True, False]),\n",
    "        'bloods': hp.choice('bloods', [True, False]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [True, False]),\n",
    "        'drug_order': hp.choice('drug_order', [True, False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [True, False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [True, False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [True, False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [True, False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [True]),\n",
    "        'hosp_site': hp.choice('hosp_site', [True]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "        'appointments': hp.choice('appointments', [False])\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast cancer sample space:\n",
    "\n",
    "space_breast_cancer = {\n",
    "    'resample': hp.choice('resample', ['undersample', 'oversample', None]),\n",
    "    'scale': hp.choice('scale', [True, False]),\n",
    "    'feature_n': hp.choice('feature_n', [ 25, 5]),\n",
    "    'param_space_size': hp.choice('param_space_size', ['medium', 'xsmall']),\n",
    "    'n_unique_out': hp.choice('n_unique_out', [10]),\n",
    "    'outcome_var_n': hp.choice('outcome_var_n', ['1']), # Optimise for alternate representations of outcome variable.\n",
    "    'percent_missing': hp.choice('percent_missing', [99, 95, 80]),\n",
    "    'corr': hp.choice('corr', [0.98, 0.85, 0.5, 0.25]),\n",
    "    'data': {\n",
    "        'age': hp.choice('age', [False]),\n",
    "        'sex': hp.choice('sex', [ False]),\n",
    "        'bmi': hp.choice('bmi', [False]),\n",
    "        'ethnicity': hp.choice('ethnicity', [ False]),\n",
    "        'bloods': hp.choice('bloods', [True, ]),\n",
    "        'diagnostic_order': hp.choice('diagnostic_order', [ False]),\n",
    "        'drug_order': hp.choice('drug_order', [ False]),\n",
    "        'annotation_n': hp.choice('annotation_n', [ False]),\n",
    "        'meta_sp_annotation_n': hp.choice('meta_sp_annotation_n', [ False]),\n",
    "        'annotation_mrc_n': hp.choice('annotation_mrc_n', [ False]),\n",
    "        'meta_sp_annotation_mrc_n': hp.choice('meta_sp_annotation_mrc_n', [ False]),\n",
    "        'core_02': hp.choice('core_02', [False]),\n",
    "        'bed': hp.choice('bed', [False]),\n",
    "        'vte_status': hp.choice('vte_status', [False]),\n",
    "        'hosp_site': hp.choice('hosp_site', [False]),\n",
    "        'core_resus': hp.choice('core_resus', [False]),\n",
    "        'news': hp.choice('news', [False]),\n",
    "        'date_time_stamp': hp.choice('date_time_stamp', [False]),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.logger_setup import setup_logger\n",
    "import logging\n",
    "\n",
    "enable_logging = False\n",
    "\n",
    "if(enable_logging):\n",
    "    logger = logging.getLogger('matplotlib.font_manager')\n",
    "\n",
    "# Set the logging level to suppress debug messages\n",
    "    logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally exclude model classes\n",
    "\n",
    "model_class_dict = {\n",
    "        \"LogisticRegression_class\": True,\n",
    "        \"knn_classifiers_class\": True,\n",
    "        \"quadratic_discriminant_analysis_class\": True,\n",
    "        \"SVC_class\": True,\n",
    "        \"XGB_class_class\": True,\n",
    "        \"mlp_classifier_class\": True,\n",
    "        \"RandomForestClassifier_class\": True,\n",
    "        \"GradientBoostingClassifier_class\": True,\n",
    "        \"CatBoost_class\": True,\n",
    "        \"GaussianNB_class\": True,\n",
    "        \"LightGBMClassifierWrapper\": True,\n",
    "        \"adaboost_class\": True,\n",
    "        \"kerasClassifier_class\": True,\n",
    "        \"knn__gpu_wrapper_class\": True,\n",
    "        \"NeuralNetworkClassifier_class\": True,\n",
    "        \"TabTransformer_class\": False,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ml_grid\n",
    "import pathlib\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from ml_grid.model_classes.h2o_classifier_class import h2o_classifier_class\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "\n",
    "from ml_grid.pipeline.data import pipe\n",
    "\n",
    "from ml_grid.util.param_space import ParamSpace\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "base_project_dir_global = 'HFE_ML_experiments/'\n",
    "\n",
    "multiple_outcomes_example = True\n",
    "\n",
    "if(enable_logging):\n",
    "    logger = setup_logger(log_folder_path = base_project_dir_global)\n",
    "\n",
    "    # Create a logger\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # Add a filter to exclude logs not related to numba.core.byteflow\n",
    "    class ByteflowFilter(logging.Filter):\n",
    "        def filter(self, record):\n",
    "            return record.name.startswith('numba.core.byteflow')\n",
    "\n",
    "    # Add the filter to the logger\n",
    "    logger.addFilter(ByteflowFilter())\n",
    "\n",
    "pathlib.Path(base_project_dir_global).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "st_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%I-%M-%S_%p\")\n",
    "\n",
    "base_project_dir = 'HFE_ML_experiments/' + st_time + \"/\"\n",
    "additional_naming = \"HFE_ML_Grid_\"\n",
    "\n",
    "print(base_project_dir)\n",
    "\n",
    "pathlib.Path(base_project_dir).mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "if(multiple_outcomes_example == False):\n",
    "    input_csv_path = 'breast_cancer_dataset.csv'\n",
    "\n",
    "else:\n",
    "    input_csv_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "\n",
    "#input_csv_path = os.path.join('..', 'gloabl_files', 'ml_binary_classification_gridsearch_hyperOpt', 'notebooks' ,'test_data_hfe_1yr_m_small.csv') #large\n",
    "\n",
    "#init csv to store each local projects results\n",
    "\n",
    "project_score_save_class(base_project_dir)\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "grid_iter_obj = grid_param_space.Grid(sample_n=n_iter).settings_list_iterator\n",
    "\n",
    "\n",
    "def objective(local_param_dict, outcome_var=None):\n",
    "    clear_output()\n",
    "    #get settings from iterator over grid of settings space\n",
    "    #local_param_dict = next(grid_iter_obj)\n",
    "    print(local_param_dict)\n",
    "    \n",
    "    #init random number string\n",
    "    \n",
    "    idx = random.randint(0,999999999999999999999)\n",
    "\n",
    "    #create object from settings\n",
    "    ml_grid_object = pipe(input_csv_path,\n",
    "                                                drop_term_list=['chrom', 'hfe', 'phlebo'],\n",
    "                                                local_param_dict=local_param_dict,\n",
    "                                                base_project_dir = base_project_dir,\n",
    "                                                additional_naming = additional_naming,\n",
    "                                                test_sample_n = 0,\n",
    "                                                param_space_index = idx,\n",
    "                                                model_class_dict = model_class_dict,\n",
    "                                                outcome_var_override = outcome_var\n",
    "                                                #outcome_var_override = None #override outcome var, example = 'outcome_var_myeloma'\n",
    "                                                #outcome_var_override = outcome_var_list[outcome_index] # set if multi class ##deprecated\n",
    "                                                )\n",
    "\n",
    "    from ml_grid.pipeline import main\n",
    "    \n",
    "    \n",
    "    # from ml_grid.model_classes.h2o_classifier_class import h2o_classifier_class\n",
    "\n",
    "    # Example overwrite/append model_class list\n",
    "    # temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "    # ml_grid_object.model_class_list = [h2o_classifier_class(\n",
    "    #             X=ml_grid_object.X_train,\n",
    "    #             y=ml_grid_object.y_train,\n",
    "    #             parameter_space_size=temp_param_space_size,\n",
    "    #         )]\n",
    "\n",
    "    # Example append \n",
    "    # if(ml_grid_object.time_series_mode == False):\n",
    "    #temp_param_space_size = ParamSpace(ml_grid_object.local_param_dict.get(\"param_space_size\"))\n",
    "\n",
    "    #     ml_grid_object.model_class_list.extend([h2o_classifier_class(\n",
    "    #                 X=ml_grid_object.X_train,\n",
    "    #                 y=ml_grid_object.y_train,\n",
    "    #                 parameter_space_size=temp_param_space_size,\n",
    "    #             )])\n",
    "\n",
    "    #pass object to be evaluated and write results to csv\n",
    "    errors, highest_score = main.run(ml_grid_object, local_param_dict=local_param_dict).execute()\n",
    "    \n",
    "    results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')\n",
    "    \n",
    "    #highest_metric_from_run = results_df[results_df['i'] == str(idx)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "    \n",
    "    highest_metric_from_run = highest_score # for hyperopt multi procesess #AUC\n",
    "    \n",
    "    #display(results_df[results_df['i'] == str(idx)].sort_values(by='auc').iloc[0])\n",
    "    \n",
    "    result = {\n",
    "        \"loss\": 1-float(highest_metric_from_run),\n",
    "        \"status\": \"ok\"  # Indicate that the evaluation was successful\n",
    "    }\n",
    "    return result\n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective(next(grid_iter_obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "# print all attributes and their values\n",
    "print(vars(global_parameters))\n",
    "\n",
    "if global_parameters.debug_level > 1:\n",
    "        print(\"Debug Mode: Additional logging enabled.\")\n",
    "\n",
    "# Update global parameters\n",
    "#global_parameters.update_parameters(debug_level=0, grid_n_jobs = -1, error_raise = True, max_param_space_iter_value=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "#print all attributes and their values\n",
    "\n",
    "print(vars(global_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')\n",
    "    \n",
    "# highest_metric_from_run = results_df[results_df['i'] == str(900424809465212743016)].sort_values(by='auc')['auc'].iloc[-1]\n",
    "\n",
    "# highest_metric_from_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun\n",
    "if( multiple_outcomes_example == False):\n",
    "\n",
    "    # Fix the additional argument (outcome_var) using partial\n",
    "    outcome_var = 'outcome_var_1'  # Define your outcome_var\n",
    "    objective_with_outcome = partial(objective, outcome_var=outcome_var)\n",
    "\n",
    "    # Initialize Trials object to store results\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the optimization\n",
    "    best = fmin(\n",
    "        fn=objective_with_outcome,  # Use the partial function\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=1,\n",
    "        trials=trials,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    results_df = pd.read_csv(base_project_dir + 'final_grid_score_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    results_df.sort_values('auc', ascending=False).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    results_df.sort_values('auc', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == True):\n",
    "    \n",
    "    dft = pd.read_csv('test_data_hfe_1yr_m_small_multiclass.csv', nrows=1)\n",
    "    dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outcome variables by finding prefix \"outcome_var_\" in column list\n",
    "\n",
    "if( multiple_outcomes_example == True):\n",
    "    outcome_var_list = [dft.columns[i] for i in range(len(dft.columns)) if \"outcome_var_\" in dft.columns[i]]\n",
    "\n",
    "    outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple outcomes one vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun\n",
    "if( multiple_outcomes_example == True):\n",
    "    \n",
    "    import multiprocessing\n",
    "    from datetime import datetime\n",
    "    from hyperopt import fmin, tpe, Trials\n",
    "    from joblib import Parallel, delayed\n",
    "    from joblib import parallel_backend  # Correct import for parallel_backend\\n\n",
    "\n",
    "    # Get number of cores\n",
    "    num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "    def process_single_outcome(outcome_index, outcome_var_list):\n",
    "        \"\"\"Process a single outcome index using multiprocessing.\"\"\"\n",
    "        outcome_var = outcome_var_list[outcome_index]\n",
    "        start_time = datetime.now()\n",
    "        print(f\"[{start_time}] Starting outcome {outcome_index}: {outcome_var}\")\n",
    "\n",
    "        # Wrap objective to include the outcome_var\n",
    "        def objective_with_outcome(params):\n",
    "            print(f\"Evaluating params: {params} for outcome {outcome_var}\")\n",
    "            return objective(params, outcome_var)\n",
    "\n",
    "        try:\n",
    "            # Use joblib's multiprocessing backend for scikit-learn operations\n",
    "            with parallel_backend('multiprocessing', n_jobs=1):\n",
    "                best = fmin(\n",
    "                    fn=objective_with_outcome,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=1,\n",
    "                    trials=Trials(),\n",
    "                    verbose=0\n",
    "                )\n",
    "            end_time = datetime.now()\n",
    "            print(f\"[{end_time}] Finished outcome {outcome_index} (Duration: {end_time - start_time})\")\n",
    "            return outcome_index, best, None\n",
    "       \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in outcome {outcome_var}: {str(e)}\")\n",
    "            #raise e\n",
    "            return outcome_index, None, str(e)\n",
    "\n",
    "    # Main execution\n",
    "    if __name__ == \"__main__\":\n",
    "        start_total = datetime.now()\n",
    "        print(f\"Starting all optimizations at {start_total}\")\n",
    "\n",
    "        # Use joblib's Parallel for multiprocessing\n",
    "        results = Parallel(n_jobs=num_cores)(\n",
    "            delayed(process_single_outcome)(i, outcome_var_list)\n",
    "            for i in range(len(outcome_var_list))\n",
    "        )\n",
    "\n",
    "        # Process results\n",
    "        for outcome_index, best, error in results:\n",
    "            if error:\n",
    "                print(f\"Exception on fmin for {outcome_var_list[outcome_index]}: {error}\")\n",
    "            elif best is not None:\n",
    "                print(f\"Best parameters for {outcome_var_list[outcome_index]}: {best}\")\n",
    "            else:\n",
    "                print(f\"No result for {outcome_var_list[outcome_index]}\")\n",
    "\n",
    "        end_total = datetime.now()\n",
    "        print(f\"\\nCompleted all optimizations at {end_total}\")\n",
    "        print(f\"Total duration: {end_total - start_total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the parent directory\n",
    "parent_dir = 'HFE_ML_experiments'\n",
    "\n",
    "# List all folders in the parent directory that match the date pattern\n",
    "folders = [f for f in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, f))]\n",
    "\n",
    "# Parse folder names as dates and find the latest one\n",
    "def parse_date(folder_name):\n",
    "    try:\n",
    "        return datetime.strptime(folder_name, '%Y-%m-%d_%I-%M-%S_%p')\n",
    "    except ValueError:\n",
    "        return None  # Skip folders that don't match the format\n",
    "\n",
    "# Filter and sort folders by date\n",
    "folders_with_dates = [(f, parse_date(f)) for f in folders]\n",
    "folders_with_dates = [f for f in folders_with_dates if f[1] is not None]\n",
    "latest_folder = max(folders_with_dates, key=lambda x: x[1])[0]  # Get the folder with the latest date\n",
    "\n",
    "print(\"latest_folder\",latest_folder)\n",
    "\n",
    "# Construct the path to the CSV file in the latest folder\n",
    "csv_path = os.path.join(parent_dir, latest_folder, 'final_grid_score_log.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sort the DataFrame by 'auc' column in descending order\n",
    "df = df.sort_values(by='auc', ascending=False)\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# group by outcome_variable and display the first row of each group with the highest auc\n",
    "df = df.groupby('outcome_variable').apply(lambda x: x.iloc[0])\n",
    "# Display the result\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Shape of the dataset: {data.shape}\")\n",
    "print(f\"Columns: {data.columns.tolist()}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for constant features\n",
    "print(\"\\n=== Constant Features ===\")\n",
    "constant_features = [col for col in data.columns if data[col].nunique() == 1]\n",
    "print(f\"Constant features: {constant_features}\")\n",
    "\n",
    "# Check for features with very low variance (almost constant)\n",
    "print(\"\\n=== Low Variance Features ===\")\n",
    "low_variance_features = []\n",
    "for col in data.columns:\n",
    "    if data[col].dtype in [np.float64, np.int64]:  # Check only numeric features\n",
    "        if data[col].std() < 0.01:  # Threshold for low variance\n",
    "            low_variance_features.append(col)\n",
    "print(f\"Low variance features: {low_variance_features}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\n=== Duplicate Rows ===\")\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for class distribution (if it's a classification problem)\n",
    "if 'target' in data.columns:  # Replace 'target' with your actual target column name\n",
    "    print(\"\\n=== Class Distribution ===\")\n",
    "    print(data['target'].value_counts())\n",
    "\n",
    "# Check for categorical features with high cardinality\n",
    "print(\"\\n=== High Cardinality Categorical Features ===\")\n",
    "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
    "high_cardinality_features = [col for col in categorical_features if data[col].nunique() > 100]\n",
    "print(f\"High cardinality categorical features: {high_cardinality_features}\")\n",
    "\n",
    "# Check for outliers in numeric features (using IQR)\n",
    "print(\"\\n=== Outliers in Numeric Features ===\")\n",
    "numeric_features = data.select_dtypes(include=[np.float64, np.int64]).columns\n",
    "for col in numeric_features:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {col}: {len(outliers)} rows\")\n",
    "\n",
    "# Summary of issues\n",
    "print(\"\\n=== Summary of Issues ===\")\n",
    "if missing_values.any():\n",
    "    print(f\"- Missing values found in {missing_values[missing_values > 0].index.tolist()}\")\n",
    "if constant_features:\n",
    "    print(f\"- Constant features found: {constant_features}\")\n",
    "if low_variance_features:\n",
    "    print(f\"- Low variance features found: {low_variance_features}\")\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"- Duplicate rows found: {duplicate_rows}\")\n",
    "if high_cardinality_features:\n",
    "    print(f\"- High cardinality categorical features found: {high_cardinality_features}\")\n",
    "if not missing_values.any() and not constant_features and not low_variance_features and not duplicate_rows and not high_cardinality_features:\n",
    "    print(\"- No major issues found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the parent directory\n",
    "parent_dir = 'HFE_ML_experiments'\n",
    "\n",
    "# List all folders in the parent directory that match the date pattern\n",
    "folders = [f for f in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, f))]\n",
    "\n",
    "# Parse folder names as dates and find the latest one\n",
    "def parse_date(folder_name):\n",
    "    try:\n",
    "        return datetime.strptime(folder_name, '%Y-%m-%d_%I-%M-%S_%p')\n",
    "    except ValueError:\n",
    "        return None  # Skip folders that don't match the format\n",
    "\n",
    "# Filter and sort folders by date\n",
    "folders_with_dates = [(f, parse_date(f)) for f in folders]\n",
    "folders_with_dates = [f for f in folders_with_dates if f[1] is not None]\n",
    "latest_folder = max(folders_with_dates, key=lambda x: x[1])[0]  # Get the folder with the latest date\n",
    "\n",
    "print(\"latest_folder\",latest_folder)\n",
    "\n",
    "# Construct the path to the CSV file in the latest folder\n",
    "csv_path = os.path.join(parent_dir, latest_folder, 'final_grid_score_log.csv')\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sort the DataFrame by 'auc' column in descending order\n",
    "df = df.sort_values(by='auc', ascending=False)\n",
    "\n",
    "print(len(df))\n",
    "# Display the result\n",
    "df.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "\n",
    "# Get the top result for each outcome_variable by AUC\n",
    "top_auc_per_outcome = df.loc[df.groupby('outcome_variable')['auc'].idxmax()]\n",
    "\n",
    "# Sort by AUC for better visualization\n",
    "top_auc_per_outcome = top_auc_per_outcome.sort_values(by='auc', ascending=False)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create barplot to show the top AUC for each outcome_variable\n",
    "sns.barplot(\n",
    "    x='auc', \n",
    "    y='outcome_variable', \n",
    "    data=top_auc_per_outcome, \n",
    "    hue='nb_size', \n",
    "    dodge=False, \n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top AUC for Each Outcome Variable')\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Outcome Variable')\n",
    "plt.legend(title='num features')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes\n",
    "from ml_grid.results_processing.core import ResultsAggregator\n",
    "from ml_grid.results_processing.plot_master import MasterPlotter\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load your data using the ResultsAggregator\n",
    "#    Replace with the actual path to your results and feature names file.\n",
    "#    The feature_names_csv is optional but required for feature-related plots.\n",
    "try:\n",
    "    aggregator = ResultsAggregator(\n",
    "        root_folder='HFE_ML_experiments',\n",
    "        feature_names_csv='test_data_hfe_1yr_m_small_multiclass.csv')\n",
    "    results_df = aggregator.aggregate_all_runs()\n",
    "\n",
    "    # 2. Instantiate the MasterPlotter with your data\n",
    "    master_plotter = MasterPlotter(results_df)\n",
    "\n",
    "    # 3. Call the plot_all() method to generate all visualizations\n",
    "    #    You can customize the primary metric and other options.\n",
    "    master_plotter.plot_all(metric='auc', stratify_by_outcome=True)\n",
    "\n",
    "except (ValueError, FileNotFoundError) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure your results folder path is correct and contains valid run data.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_grid_env)",
   "language": "python",
   "name": "ml_grid_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
