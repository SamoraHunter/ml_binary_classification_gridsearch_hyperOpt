{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from IPython.display import display\n",
    "\n",
    "# --- Make sure your notebook's working directory is the project root ---\n",
    "# (e.g., /home/samorah/_data/ml_binary_classification_gridsearch_hyperOpt/)\n",
    "from pathlib import Path\n",
    "# so that the imports work correctly.\n",
    "from ml_grid.util.synthetic_data_generator import generate_synthetic_data\n",
    "from ml_grid.util.impute_data_for_pipe import save_missing_percentage, mean_impute_dataframe\n",
    "\n",
    "# --- 1. Setup Logging ---\n",
    "# This allows you to see the informative output from the generator and other steps.\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "\n",
    "# --- 2. Generate Synthetic Data and the Ground-Truth Map ---\n",
    "logging.info(\"Generating a sample synthetic dataset using the importable function...\")\n",
    "synthetic_df, important_feature_map = generate_synthetic_data(\n",
    "    n_rows=200,\n",
    "    n_features=30,\n",
    "    n_outcome_vars=3,\n",
    "    feature_strength=0.7,\n",
    "    percent_important_features=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Generated DataFrame Info (Before Imputation) ---\")\n",
    "print(f\"Shape: {synthetic_df.shape}\")\n",
    "print(f\"Total NaNs present: {synthetic_df.isnull().sum().sum()}\")\n",
    "print(\"Sample of data with missing values:\")\n",
    "display(synthetic_df.head())\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- 3. Calculate and Save the Percentage of Missing Values ---\n",
    "missing_pickle_filename = \"percent_missing_synthetic_data_generated.pkl\"\n",
    "print(f\"\\nCalculating missing value percentages and saving to '{missing_pickle_filename}'...\")\n",
    "save_missing_percentage(synthetic_df, output_file=missing_pickle_filename)\n",
    "print(\"✅ Missing value pickle file saved.\")\n",
    "\n",
    "\n",
    "# --- 4. Perform Mean Imputation ---\n",
    "print(\"\\nPerforming mean imputation on the dataset...\")\n",
    "# Get the list of outcome columns to exclude them from imputation\n",
    "outcome_columns = list(important_feature_map.keys())\n",
    "imputed_df = mean_impute_dataframe(data=synthetic_df.copy(), y_vars=outcome_columns)\n",
    "print(f\"Imputation complete. NaNs present after imputation: {imputed_df.isnull().sum().sum()}\")\n",
    "print(\"✅ Mean imputation successful.\")\n",
    "\n",
    "\n",
    "# --- 5. Save the Imputed Data to the Final CSV File ---\n",
    "output_csv_filename = \"synthetic_data_generated.csv\"\n",
    "imputed_df.to_csv(output_csv_filename, index=False)\n",
    "print(f\"\\nImputed data saved to '{output_csv_filename}'\")\n",
    "print(\"✅ Final CSV file saved.\")\n",
    "\n",
    "print(\"\\n--- Final Imputed DataFrame ---\")\n",
    "display(imputed_df.head())\n",
    "print(\"-------------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Global Configuration\n",
    "\n",
    "Load `config_hyperopt.yml` once at the beginning to ensure all subsequent cells can access global settings and paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "CONFIG_HYPEROPT_PATH = Path(\"../config_hyperopt.yml\")\n",
    "\n",
    "if not CONFIG_HYPEROPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Hyperopt configuration file not found at {CONFIG_HYPEROPT_PATH.resolve()}\")\n",
    "\n",
    "with open(CONFIG_HYPEROPT_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f) # Use 'config' as the main variable for the search\n",
    "print(\"  Hyperopt configuration loaded successfully.\")\n",
    "\n",
    "# Define project root (assuming notebook is run from project root)\n",
    "project_root = Path.cwd()\n",
    "print(f\"Project root set to: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal standalone script to instantiate and test the ml_grid.pipeline.data.pipe class.\n",
    "\n",
    "This script provides a clear example of the minimum setup required to create\n",
    "an `ml_grid_object`. It is intended for debugging the data pipeline in isolation.\n",
    "\"\"\"\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# --- Essential imports from your ml_grid project ---\n",
    "# Ensure your project is installed (e.g., `pip install -e .`) or the path is configured\n",
    "# so these imports can be found.\n",
    "try:\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components.\")\n",
    "    print(\"Please ensure you are running this script from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. LOAD CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"1. Loading configuration from config.yml...\")\n",
    "CONFIG_PATH = project_root / \"../config_single_run.yml\"\n",
    "\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Configuration file not found at {CONFIG_PATH.resolve()}\")\n",
    "\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config_single_run = yaml.safe_load(f)\n",
    "print(\"  Configuration loaded successfully.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. SETUP ENVIRONMENT & PATHS FROM CONFIG\n",
    "# =============================================================================\n",
    "print(\"\\n2. Setting up environment and paths...\")\n",
    "\n",
    "# Project root is the current working directory\n",
    "# project_root is already defined from config_hyperopt.yml loading\n",
    "base_project_dir = str(project_root)\n",
    "print(f\"  Project Root: {project_root}\")\n",
    "\n",
    "# Load paths from config\n",
    "input_csv_path = project_root / config_single_run['data']['file_path']\n",
    "if not input_csv_path.exists():\n",
    "    print(f\"  ERROR: Data file not found at '{input_csv_path}'\")\n",
    "    print(\"  Please make sure the path is correct.\")\n",
    "    exit()\n",
    "print(f\"  Input CSV: {input_csv_path.resolve()}\")\n",
    "\n",
    "# Create experiment directory from config\n",
    "experiments_base_dir = project_root / config_single_run['experiment']['experiments_base_dir']\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=experiments_base_dir,\n",
    "    additional_naming=config_single_run['experiment']['additional_naming']\n",
    ")\n",
    "experiment_dir = Path(experiment_dir) # Ensure it's a Path object\n",
    "print(f\"  Experiment Directory: {experiment_dir}\")\n",
    "\n",
    "# Configure global parameters from config\n",
    "global_parameters.verbose = config_single_run['global_params']['verbose']\n",
    "global_parameters.error_raise = config_single_run['global_params']['error_raise']\n",
    "\n",
    "# =============================================================================\n",
    "# 3. DEFINE PIPELINE PARAMETERS FROM CONFIG\n",
    "# =============================================================================\n",
    "print(\"\\n3. Defining pipeline parameters...\")\n",
    "\n",
    "drop_term_list = config_single_run['data']['drop_term_list']\n",
    "print(f\"  Drop terms: {drop_term_list}\")\n",
    "\n",
    "model_class_dict = config_single_run['models']\n",
    "print(f\"  Enabled models: {[k for k, v in model_class_dict.items() if v]}\")\n",
    "\n",
    "outcome_var = config_single_run['data'].get('outcome_var_override') # Use .get() for safety\n",
    "print(f\"  Outcome variable override: '{outcome_var}'\")\n",
    "\n",
    "local_param_dict = config_single_run['run_params']\n",
    "print(\"  Local parameter dictionary configured.\")\n",
    "\n",
    "# A unique index for this parameter combination (useful when iterating)\n",
    "param_space_index = 0\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. INSTANTIATE THE PIPE CLASS\n",
    "# =============================================================================\n",
    "print(\"\\n4. Instantiating the 'pipe' class...\")\n",
    "\n",
    "try:\n",
    "    # This is the call to create the ml_grid_object.\n",
    "    # The entire data pipeline runs during this initialization.\n",
    "    ml_grid_object = pipe(\n",
    "        file_name=str(input_csv_path.resolve()),\n",
    "        drop_term_list=drop_term_list,\n",
    "        local_param_dict=local_param_dict,\n",
    "        base_project_dir=base_project_dir,\n",
    "        experiment_dir=experiment_dir,\n",
    "        test_sample_n=0,  # Use 0 to process the full dataset\n",
    "        param_space_index=param_space_index,\n",
    "        model_class_dict=model_class_dict,\n",
    "        outcome_var_override=outcome_var\n",
    "    )\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS: 'ml_grid_object' created successfully.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # =========================================================================\n",
    "    # 5. INSPECT THE RESULTS\n",
    "    # =========================================================================\n",
    "    print(\"\\n5. Inspecting the final object attributes:\")\n",
    "    print(f\"  - Outcome Variable Used: {ml_grid_object.outcome_variable}\")\n",
    "    print(f\"  - X_train shape: {ml_grid_object.X_train.shape}\")\n",
    "    print(f\"  - y_train shape: {ml_grid_object.y_train.shape}\")\n",
    "    print(f\"  - X_test shape: {ml_grid_object.X_test.shape}\")\n",
    "    print(f\"  - y_test shape: {ml_grid_object.y_test.shape}\")\n",
    "    print(f\"  - Number of final features: {len(ml_grid_object.final_column_list)}\")\n",
    "    print(f\"  - Number of available models: {len(ml_grid_object.model_class_list)}\")\n",
    "    print(\"\\nFinal X_train columns sample:\")\n",
    "    print(ml_grid_object.X_train.columns.to_list()[:10])\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ERROR: Failed to instantiate the 'pipe' class.\")\n",
    "    print(\"=\"*50)\n",
    "    import traceback\n",
    "    print(f\"\\nAn error of type '{type(e).__name__}' occurred: {e}\")\n",
    "    print(\"\\nFull Traceback:\")\n",
    "    traceback.print_exc()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_grid_object.X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "print(\"\\n4. Initializing and executing the model training run...\")\n",
    "\n",
    "# --- Essential imports ---\n",
    "# Make sure your project is installed or the path is configured.\n",
    "try:\n",
    "    from ml_grid.pipeline.main import run\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components for the 'run' step.\")\n",
    "    print(\"Please ensure you are running this from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Use exit() if running as a script, or just let the error show in a notebook.\n",
    "    # exit() \n",
    "\n",
    "# Check if ml_grid_object exists from the previous cell's execution\n",
    "if 'ml_grid_object' not in locals() or 'local_param_dict' not in locals():\n",
    "    print(\"\\nERROR: 'ml_grid_object' or 'local_param_dict' not found.\")\n",
    "    print(\"Please ensure you have successfully run the previous cell (the 'pipe' instantiation script) first.\")\n",
    "else:\n",
    "    # REMOVED the top-level try...except block. Any exception will now halt the cell.\n",
    "    \n",
    "    print(\"  Instantiating the 'run' class with the data object...\")\n",
    "    # Instantiate the 'run' class with the object from the data pipeline\n",
    "    run_instance = run(\n",
    "        ml_grid_object=ml_grid_object,\n",
    "        local_param_dict=local_param_dict\n",
    "    )\n",
    "\n",
    "    print(\"  Executing the hyperparameter search and cross-validation...\")\n",
    "    # Execute the hyperparameter search and cross-validation for all models.\n",
    "    # If an error occurs here, it will stop the notebook execution.\n",
    "    model_errors, highest_score = run_instance.execute()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SUCCESS: Model training and evaluation complete.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    print(\"\\n5. Inspecting the training results:\")\n",
    "    print(f\"  - Highest score achieved across all models: {highest_score:.4f}\")\n",
    "\n",
    "    if model_errors:\n",
    "        print(f\"\\n  - {len(model_errors)} model(s) encountered errors during training:\")\n",
    "        for i, error_info in enumerate(model_errors):\n",
    "            try:\n",
    "                # Try to get a meaningful name for the model\n",
    "                model_name = error_info[0].__class__.__name__\n",
    "            except:\n",
    "                model_name = \"Unknown Model\"\n",
    "            error_exception = error_info[1]\n",
    "            print(f\"    {i+1}. Model: {model_name}, Error: {error_exception}\")\n",
    "    else:\n",
    "        print(\"\\n  - All configured models ran without critical errors.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up test results directory\n",
    "!rm -r 'experiments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD HYPEROPT CONFIGURATION\n",
    "# =============================================================================\n",
    "print(\"Loading configuration for Hyperopt search from config_hyperopt.yml...\")\n",
    "CONFIG_HYPEROPT_PATH = project_root / \"../config_hyperopt.yml\"\n",
    "\n",
    "if not CONFIG_HYPEROPT_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Hyperopt configuration file not found at {CONFIG_HYPEROPT_PATH.resolve()}\")\n",
    "\n",
    "with open(CONFIG_HYPEROPT_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f) # Use 'config' as the main variable for the search\n",
    "print(\"  Hyperopt configuration loaded successfully.\")\n",
    "\n",
    "# =============================================================================\n",
    "# BUILD HYPEROPT SEARCH SPACE FROM CONFIG\n",
    "# =============================================================================\n",
    "def build_hyperopt_space(config_space):\n",
    "    \"\"\"Dynamically builds a hyperopt search space from the config dictionary.\"\"\"\n",
    "    space = {}\n",
    "    # The 'data' key is a nested dictionary of choices\n",
    "    space['data'] = {k: hp.choice(k, v) for k, v in config_space.get('data', {}).items()}\n",
    "    \n",
    "    # All other keys are simple choices, excluding the settings key\n",
    "    for key, value in config_space.items():\n",
    "        # The key 'corr' in old notebooks is now 'correlation_threshold'\n",
    "        # This ensures backward compatibility if an old space is being tested.\n",
    "        if key == 'corr':\n",
    "            space['correlation_threshold'] = hp.choice('correlation_threshold', value)\n",
    "        elif key not in ['data', 'hyperopt_settings']:\n",
    "            space[key] = hp.choice(key, value)\n",
    "            \n",
    "    return space\n",
    "\n",
    "# Build the hyperopt space object\n",
    "space = build_hyperopt_space(config['hyperopt_search_space'])\n",
    "\n",
    "print(\"\\nHyperopt search space built from config:\")\n",
    "print(space)\n",
    "\n",
    "# =============================================================================\n",
    "# LOAD OTHER PARAMETERS FROM CONFIG\n",
    "# =============================================================================\n",
    "model_class_dict = config['models']\n",
    "print(\"\\nModels to be used in Hyperopt search:\")\n",
    "print([model for model, enabled in model_class_dict.items() if enabled])\n",
    "\n",
    "# Define project root and construct absolute paths\n",
    "# project_root is already defined from config_hyperopt.yml loading\n",
    "base_project_dir = project_root / config['experiment']['experiments_base_dir']\n",
    "experiment_name = config['experiment']['additional_naming']\n",
    "multiple_outcomes_example = config['data']['multiple_outcomes']\n",
    "input_csv_path = project_root / config['data']['file_path']\n",
    "drop_term_list = config['data']['drop_term_list']\n",
    "max_evals = config['hyperopt_settings']['max_evals']\n",
    "\n",
    "print(f\"\\nExperiment settings:\")\n",
    "print(f\"  Base Directory: {base_project_dir.resolve()}\")\n",
    "print(f\"  Experiment Name: {experiment_name}\")\n",
    "print(f\"  Input CSV: {input_csv_path.resolve()}\")\n",
    "print(f\"  Multiple Outcomes: {multiple_outcomes_example}\")\n",
    "print(f\"  Max Evals per Outcome: {max_evals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import traceback\n",
    "import warnings\n",
    "from typing import Any, Dict, List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output, display\n",
    "from numpy import absolute, mean, std\n",
    "from pandas.testing import assert_index_equal\n",
    "from sklearn import metrics\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    ParameterGrid,\n",
    "    RandomizedSearchCV,\n",
    "    RepeatedKFold,\n",
    "    cross_validate,\n",
    ")\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from xgboost.core import XGBoostError\n",
    "\n",
    "# --- Essential imports from your ml_grid project ---\n",
    "# Ensure your project is installed or the path is configured.\n",
    "try:\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.pipeline.hyperparameter_search import HyperparameterSearch\n",
    "    from ml_grid.util.bayes_utils import calculate_combinations\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "    from ml_grid.util.global_params import global_parameters\n",
    "    from ml_grid.util.project_score_save import project_score_save_class\n",
    "    from ml_grid.util.validate_parameters import validate_parameters_helper\n",
    "except ImportError as e:\n",
    "    print(\"Could not import ml_grid components.\")\n",
    "    print(\"Please ensure you are running this from the project root directory,\")\n",
    "    print(\"and that the project has been installed (e.g., using 'pip install -e .').\")\n",
    "    print(f\"Error: {e}\")\n",
    "    # Use exit() if running as a script, or just let the error show in a notebook.\n",
    "    # exit()\n",
    "\n",
    "# =============================================================================\n",
    "# STANDALONE SCRIPT TO DEBUG `grid_search_crossvalidate` INTERNALS\n",
    "# =============================================================================\n",
    "\n",
    "print(f\"   Project Root: {base_project_dir}\")\n",
    "\n",
    "# --- Define the path to your input data ---\n",
    "# IMPORTANT: Update this path to the correct location of your file.\n",
    "# Now relative to the project_root\n",
    "input_csv_path = project_root / \"synthetic_data_generated.csv\"\n",
    "\n",
    "if not input_csv_path.exists():\n",
    "    print(f\"   ERROR: Data file not found at '{input_csv_path.resolve()}'\")\n",
    "    print(\"   Please make sure the path is correct.\")\n",
    "    # exit()\n",
    "else:\n",
    "    print(f\"   Input CSV: {input_csv_path.resolve()}\")\n",
    "\n",
    "# Create a directory for this specific experiment run's logs and outputs\n",
    "experiments_base_dir = project_root / config['experiment']['experiments_base_dir']\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=experiments_base_dir,\n",
    "    additional_naming=\"GSCV_Internals_Debug\"\n",
    ")\n",
    "experiment_dir = Path(experiment_dir) # Ensure it's a Path object\n",
    "print(f\"   Experiment Directory: {experiment_dir.resolve()}\")\n",
    "\n",
    "\n",
    "# --- 2. Configure Parameters ---\n",
    "print(\"\\n2. Configuring parameters...\")\n",
    "# Global parameters\n",
    "global_parameters.verbose = 1\n",
    "global_parameters.error_raise = True\n",
    "global_parameters.bayessearch = False\n",
    "global_parameters.random_grid_search = True\n",
    "global_parameters.sub_sample_param_space_pct = 0.2\n",
    "\n",
    "# Local parameters for the data pipeline, configured for your dataset\n",
    "local_param_dict = {\n",
    "    'outcome_var_n': 1,\n",
    "    'param_space_size': 'xsmall',\n",
    "    'scale': True,\n",
    "    'feature_n': 90,\n",
    "    'use_embedding': False,\n",
    "    'percent_missing': 98,\n",
    "    'correlation_threshold': 0.95,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42,\n",
    "    'data': {\n",
    "        'age': True, 'sex': True, 'bmi': True, 'ethnicity': True,\n",
    "        'bloods': True, 'diagnostic_order': True, 'drug_order': True,\n",
    "        'annotation_n': True, 'meta_sp_annotation_n': True,\n",
    "        'annotation_mrc_n': True, 'meta_sp_annotation_mrc_n': True,\n",
    "        'core_02': True, 'bed': True, 'vte_status': True,\n",
    "        'hosp_site': True, 'core_resus': True, 'news': True,\n",
    "        'date_time_stamp': False, 'appointments': False,\n",
    "    }\n",
    "}\n",
    "print(\"   Parameters configured.\")\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "ml_grid_object = None\n",
    "try:\n",
    "    # --- 3. Run Data Pipeline to Get `ml_grid_object` ---\n",
    "    print(\"\\n3. Initializing data pipeline (`pipe`) to prepare data...\")\n",
    "    ml_grid_object = pipe(\n",
    "        file_name=str(input_csv_path),\n",
    "        drop_term_list=config['data']['drop_term_list'],\n",
    "        experiment_dir=str(experiment_dir),\n",
    "        base_project_dir=base_project_dir,\n",
    "        local_param_dict=local_param_dict,\n",
    "        param_space_index=0,\n",
    "        model_class_dict=config['models'],\n",
    "        outcome_var_override='outcome_var_1'\n",
    "    )\n",
    "    print(\"   Data pipeline finished.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- 4. EXPLICIT `grid_search_crossvalidate` INTERNAL LOGIC ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n4. Executing `grid_search_crossvalidate` internal logic...\")\n",
    "    start_time_main = time.time()\n",
    "\n",
    "    # --- 4a. Select a model and extract its properties ---\n",
    "    model_to_test = ml_grid_object.model_class_list[0]\n",
    "    algorithm_implementation = model_to_test.algorithm_implementation\n",
    "    parameter_space = model_to_test.parameter_space\n",
    "    method_name = model_to_test.method_name\n",
    "    print(f\"   - Model for debugging: {method_name}\")\n",
    "\n",
    "    # --- 4b. Initialize variables from `grid_search_crossvalidate.__init__` ---\n",
    "    # CORRECTED: Set each warning filter individually.\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "    \n",
    "    grid_n_jobs = global_parameters.grid_n_jobs\n",
    "    if \"keras\" in method_name.lower() or \"xgb\" in method_name.lower() or \"catboost\" in method_name.lower():\n",
    "        grid_n_jobs = 1\n",
    "        try:\n",
    "            gpu_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "            for device in gpu_devices:\n",
    "                tf.config.experimental.set_memory_growth(device, True)\n",
    "        except Exception as e:\n",
    "            print(f\"   - Could not configure GPU for TensorFlow: {e}\")\n",
    "\n",
    "    # Extract data from the ml_grid_object\n",
    "    X_train, y_train = ml_grid_object.X_train, ml_grid_object.y_train\n",
    "    X_test, y_test = ml_grid_object.X_test, ml_grid_object.y_test\n",
    "\n",
    "    # --- 4c. Prepare for Hyperparameter Search ---\n",
    "    max_param_space_iter_value = global_parameters.max_param_space_iter_value\n",
    "    param_grid_size = len(ParameterGrid(parameter_space))\n",
    "    sub_sample_parameter_val = int(global_parameters.sub_sample_param_space_pct * param_grid_size)\n",
    "    n_iter_v = max(2, sub_sample_parameter_val)\n",
    "    n_iter_v = min(n_iter_v, max_param_space_iter_value)\n",
    "    print(f\"   - Hyperparameter search iterations (n_iter): {n_iter_v}\")\n",
    "\n",
    "    # Instantiate the HyperparameterSearch class\n",
    "    search = HyperparameterSearch(\n",
    "        algorithm=algorithm_implementation,\n",
    "        parameter_space=parameter_space,\n",
    "        method_name=method_name,\n",
    "        global_params=global_parameters,\n",
    "        max_iter=n_iter_v,\n",
    "        ml_grid_object=ml_grid_object\n",
    "    )\n",
    "\n",
    "    # --- 4d. Run the Hyperparameter Search ---\n",
    "    print(\"   - Running HyperparameterSearch.run_search()...\")\n",
    "    # This is the core search step (e.g., RandomizedSearchCV.fit())\n",
    "    best_estimator = search.run_search(X_train, y_train)\n",
    "    print(f\"   - Best estimator found: {best_estimator.get_params()}\")\n",
    "\n",
    "    # --- 4e. Fit the Final Model and Evaluate ---\n",
    "    print(\"   - Fitting the best estimator on the full training data...\")\n",
    "    # Use numpy arrays for final fitting\n",
    "    best_estimator.fit(X_train.values, y_train.values)\n",
    "\n",
    "    # --- 4f. Run Cross-Validation on the Best Model ---\n",
    "    print(\"   - Running cross_validate on the best estimator...\")\n",
    "    cv_splitter = RepeatedKFold(n_splits=3, n_repeats=2, random_state=1)\n",
    "    \n",
    "    try:\n",
    "        scores = cross_validate(\n",
    "            best_estimator,\n",
    "            X_train.values,\n",
    "            y_train.values,\n",
    "            scoring=global_parameters.metric_list,\n",
    "            cv=cv_splitter,\n",
    "            n_jobs=grid_n_jobs,\n",
    "            error_score='raise'\n",
    "        )\n",
    "        failed = False\n",
    "        print(\"   - Cross-validation successful.\")\n",
    "        for metric, values in scores.items():\n",
    "            print(f\"     - {metric}: {np.mean(values):.4f}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"   - Cross-validation failed: {e}\")\n",
    "        scores = {metric: [0.5] for metric in global_parameters.metric_list} # Default scores\n",
    "        failed = True\n",
    "\n",
    "    # --- 4g. Predict on the Test Set and Calculate Final Score ---\n",
    "    print(\"   - Predicting on the test set...\")\n",
    "    # Use .values to ensure numpy array for prediction\n",
    "    best_pred_orig = best_estimator.predict(X_test.values)\n",
    "    \n",
    "    # The final score to be optimized/reported\n",
    "    final_auc_score = roc_auc_score(y_test, best_pred_orig)\n",
    "    \n",
    "    score_saver = project_score_save_class(experiment_dir=experiment_dir)\n",
    "\n",
    "    \n",
    "    # --- 4h. Log the results (emulating project_score_save_class) ---\n",
    "    score_saver.update_score_log(\n",
    "        ml_grid_object=ml_grid_object,\n",
    "        scores=scores,\n",
    "        best_pred_orig=best_pred_orig,\n",
    "        current_algorithm=best_estimator,\n",
    "        method_name=method_name,\n",
    "        pg=param_grid_size,\n",
    "        start=start_time_main,\n",
    "        n_iter_v=n_iter_v,\n",
    "        failed=failed\n",
    "    )\n",
    "    print(\"   - Results logged.\")\n",
    "\n",
    "    # =========================================================================\n",
    "    # --- 5. Display the Final Results ---\n",
    "    # =========================================================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUCCESS: Standalone internal logic run complete.\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\n   - Model Tested: {method_name}\")\n",
    "    print(f\"   - Final Reported AUC Score on Test Set: {final_auc_score:.4f}\")\n",
    "    print(f\"   - Total execution time: {time.time() - start_time_main:.2f} seconds\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"A CRITICAL ERROR OCCURRED DURING EXECUTION\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Error Type: {type(e).__name__}\")\n",
    "    print(f\"Error Message: {e}\")\n",
    "    print(\"\\nFull Traceback:\")\n",
    "    traceback.print_exc()\n",
    "    raise e\n",
    "\n",
    "finally:\n",
    "    # --- 6. Cleanup ---\n",
    "    try:\n",
    "        os.remove('final_grid_score_log.csv')\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        pass # File might not exist if no runs completed\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup of final_grid_score_log.csv: {e}\")\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(experiments_base_dir)\n",
    "    except FileNotFoundError:\n",
    "        pass # Directory might not exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup of experiment directory: {e}\")\n",
    "\n",
    "    try: \n",
    "        shutil.rmtree('run_0')\n",
    "    except FileNotFoundError:\n",
    "        pass # Directory might not exist\n",
    "    except Exception as e:\n",
    "        print(f\"Error during cleanup of run_0: {e}\")\n",
    "    print(\"\\n6. Clean up complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "# print all attributes and their values\n",
    "print(vars(global_parameters))\n",
    "\n",
    "if global_parameters.debug_level > 1:\n",
    "        print(\"Debug Mode: Additional logging enabled.\")\n",
    "\n",
    "# Update global parameters\n",
    "#global_parameters.update_parameters(debug_level=0, grid_n_jobs = -1, error_raise = True, max_param_space_iter_value=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_grid.util.global_params import global_parameters\n",
    "\n",
    "#print all attributes and their values\n",
    "\n",
    "print(vars(global_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_grid\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from hyperopt import STATUS_FAIL\n",
    "from ml_grid.model_classes.h2o_classifier_class import H2OAutoMLConfig\n",
    "from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "from ml_grid.util.project_score_save import project_score_save_class\n",
    "from ml_grid.pipeline.data import NoFeaturesError, pipe\n",
    "from ml_grid.util.param_space import ParamSpace\n",
    "\n",
    "random.seed(1234)\n",
    "\n",
    "# --- Setup Experiment Directory and Logging ---\n",
    "# This is the main directory for the entire Hyperopt search.\n",
    "experiment_dir = create_experiment_directory(\n",
    "    base_dir=base_project_dir, # From cell 7\n",
    "    additional_naming=experiment_name # From cell 7\n",
    ")\n",
    "experiment_dir = Path(experiment_dir)\n",
    "print(f\"Main experiment directory: {experiment_dir.resolve()}\")\n",
    "\n",
    "# Initialize the project-level score log within the main experiment directory\n",
    "project_score_save_class(experiment_dir)\n",
    "\n",
    "def objective(local_param_dict, outcome_var=None):\n",
    "    \"\"\"The objective function that Hyperopt will minimize.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Evaluating for outcome: {outcome_var}\")\n",
    "    print(f\"Params: {local_param_dict}\")\n",
    "    \n",
    "    # A unique ID for this specific trial\n",
    "    trial_idx = random.randint(0, 9999999999)\n",
    "\n",
    "    try:\n",
    "        ml_grid_object = pipe(\n",
    "            file_name=str(input_csv_path.resolve()),\n",
    "            drop_term_list=drop_term_list,\n",
    "            local_param_dict=local_param_dict,\n",
    "            base_project_dir=base_project_dir,\n",
    "            experiment_dir=experiment_dir,  \n",
    "            test_sample_n=0,\n",
    "            param_space_index=trial_idx,\n",
    "            model_class_dict=model_class_dict,\n",
    "            outcome_var_override=outcome_var\n",
    "        )\n",
    "\n",
    "        from ml_grid.pipeline import main\n",
    "        \n",
    "        # Run the modeling pipeline and get the best score for this trial\n",
    "        errors, highest_score = main.run(ml_grid_object, local_param_dict=local_param_dict).execute()\n",
    "        \n",
    "        return {\n",
    "            'loss': 1 - float(highest_score), # Hyperopt minimizes, so we use 1 - AUC\n",
    "            'status': 'ok'\n",
    "        }\n",
    "    except NoFeaturesError as e:\n",
    "        print(f\"Skipping trial due to NoFeaturesError: {e}\")\n",
    "        return {'status': STATUS_FAIL, 'loss': float('inf')}\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise e\n",
    "        return {'status': STATUS_FAIL, 'loss': float('inf')}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%prun\n",
    "if( multiple_outcomes_example == False):\n",
    "\n",
    "    # Fix the additional argument (outcome_var) using partial\n",
    "    # Define the single outcome to test\n",
    "    single_outcome_var = 'outcome_var_1'\n",
    "    objective_with_outcome = partial(objective, outcome_var=single_outcome_var)\n",
    "\n",
    "    # Initialize Trials object to store results\n",
    "    trials = Trials()\n",
    "\n",
    "    # Run the optimization\n",
    "    best = fmin(\n",
    "        fn=objective_with_outcome,  # Use the partial function\n",
    "        space=space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=max_evals,\n",
    "        trials=trials,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    # Assuming the log file is in the current experiment_dir\n",
    "    results_df = pd.read_csv(experiment_dir / 'final_grid_score_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    if 'results_df' in locals():\n",
    "        display(results_df.sort_values('auc', ascending=False).iloc[0])\n",
    "    else:\n",
    "        print(\"results_df not found. Please run the previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == False):\n",
    "    if 'results_df' in locals():\n",
    "        display(results_df.sort_values('auc', ascending=False))\n",
    "    else:\n",
    "        print(\"results_df not found. Please run the previous cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if( multiple_outcomes_example == True):\n",
    "    \n",
    "    dft = pd.read_csv(input_csv_path.resolve(), nrows=1)\n",
    "    dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get outcome variables by finding prefix \"outcome_var_\" in column list\n",
    "\n",
    "if( multiple_outcomes_example == True):\n",
    "    outcome_var_list = [dft.columns[i] for i in range(len(dft.columns)) if \"outcome_var_\" in dft.columns[i]]\n",
    "\n",
    "    outcome_var_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%prun\n",
    "if multiple_outcomes_example:\n",
    "    import multiprocessing\n",
    "    from datetime import datetime\n",
    "    from functools import partial\n",
    "    from hyperopt import fmin, tpe, Trials, STATUS_OK, hp\n",
    "    from joblib import Parallel, delayed\n",
    "    import traceback\n",
    "    import sys\n",
    "    import pandas as pd\n",
    "    from pathlib import Path\n",
    "\n",
    "    # --- Imports from your ml_grid project ---\n",
    "    from ml_grid.pipeline.data import pipe\n",
    "    from ml_grid.pipeline.main import run\n",
    "    from ml_grid.util.create_experiment_directory import create_experiment_directory\n",
    "    # Assuming config_parser is in notebooks/ or a discoverable path\n",
    "    from config_parser import load_config\n",
    "\n",
    "    # 1. --- Load Configuration from YAML ---\n",
    "    # This assumes 'config_hyperopt.yml' is in a location accessible from the notebook\n",
    "    config = load_config('config_hyperopt.yml')\n",
    "\n",
    "    # 2. --- Set up Experiment Environment ---\n",
    "    project_root = Path.cwd() # Or specify the correct project root path\n",
    "    experiments_base_dir = project_root / config['experiment']['experiments_base_dir']\n",
    "    experiment_dir = create_experiment_directory(\n",
    "        base_dir=experiments_base_dir,\n",
    "        additional_naming=config['experiment']['additional_naming']\n",
    "    )\n",
    "\n",
    "    # 3. --- Dynamically Build Hyperopt Search Space from Config ---\n",
    "    space = {}\n",
    "    for key, value in config['hyperopt_search_space'].items():\n",
    "        if key == 'data':\n",
    "            # Handle nested 'data' dictionary for feature groups\n",
    "            space['data'] = {k: hp.choice(f'data_{k}', v) for k, v in value.items()}\n",
    "        else:\n",
    "            space[key] = hp.choice(key, value)\n",
    "\n",
    "    # 4. --- Get Hyperopt Settings from Config ---\n",
    "    max_evals = config['hyperopt_settings']['max_evals']\n",
    "\n",
    "    # 5. --- Determine Outcome Variables ---\n",
    "    outcome_var_list = []\n",
    "    if config['data'].get('multiple_outcomes', False):\n",
    "        # Use an absolute path if the file is not in the current working directory\n",
    "        data_file_path = config['data']['file_path']\n",
    "        if not Path(data_file_path).is_absolute():\n",
    "            data_file_path = project_root / data_file_path\n",
    "        \n",
    "        # Let FileNotFoundError propagate naturally\n",
    "        df = pd.read_csv(data_file_path)\n",
    "        outcome_var_list = [col for col in df.columns if 'outcome_var_' in col]\n",
    "        \n",
    "        if not outcome_var_list:\n",
    "            raise ValueError(f\"No outcome variables found with 'outcome_var_' prefix in {data_file_path}\")\n",
    "        \n",
    "        print(f\"Found {len(outcome_var_list)} outcome variables to process.\")\n",
    "    else:\n",
    "        # Use the default from the config if multiple_outcomes is false\n",
    "        outcome_var_list = config['hyperopt_search_space']['outcome_var_n']\n",
    "\n",
    "    # 6. --- Define the Objective Function for Hyperopt ---\n",
    "    def objective(params, outcome_var):\n",
    "        \"\"\"\n",
    "        Objective function for hyperopt. It receives sampled parameters\n",
    "        and the specific outcome variable for the current run.\n",
    "        \n",
    "        NOTE: Exceptions are caught here only to return a failed trial status\n",
    "        to hyperopt, but critical errors should still halt execution.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # The 'params' dict contains the sampled hyperparameters.\n",
    "            # We use it to build the local_param_dict for the pipeline.\n",
    "            local_param_dict = params\n",
    "\n",
    "            # Initialize the data pipeline and the main 'run' class instance.\n",
    "            # This happens for every trial, which is necessary because the\n",
    "            # data processing steps (scaling, feature selection, etc.)\n",
    "            # are part of the hyperparameter search.\n",
    "            \n",
    "            # Correctly instantiate the 'pipe' object by unpacking the config\n",
    "            ml_grid_object = pipe(\n",
    "                file_name=config['data']['file_path'],\n",
    "                drop_term_list=config['data']['drop_term_list'],\n",
    "                model_class_dict=config['models'],\n",
    "                local_param_dict=local_param_dict,\n",
    "                base_project_dir=project_root,\n",
    "                experiment_dir=experiment_dir,\n",
    "                param_space_index=0, # Index is less relevant for hyperopt\n",
    "                outcome_var_override=outcome_var\n",
    "            )\n",
    "\n",
    "            # The 'run' class's execute() method will loop through all models\n",
    "            # enabled in the config and evaluate them with the current data pipeline setup.\n",
    "            run_instance = run(local_param_dict=local_param_dict, ml_grid_object=ml_grid_object)\n",
    "            \n",
    "            # The execute method returns the list of errors and the highest score\n",
    "            # achieved by any model in that trial.\n",
    "            _, highest_score = run_instance.execute()\n",
    "\n",
    "            # Hyperopt minimizes 'loss', so we return the negative of the highest score.\n",
    "            return {'loss': -highest_score, 'status': STATUS_OK, 'params': params}\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log the error for debugging\n",
    "            tb_str = traceback.format_exc()\n",
    "            print(f\"ERROR in objective for {outcome_var} with params {params}: {e}\\n{tb_str}\", file=sys.stderr)\n",
    "            \n",
    "            # Return failure status to hyperopt (this allows it to try other params)\n",
    "            # For truly critical errors, you could raise instead\n",
    "            return {'loss': float('inf'), 'status': 'fail', 'message': str(e)}\n",
    "\n",
    "    # 7. --- Define the Worker Function for Parallel Processing ---\n",
    "    def process_single_outcome(outcome_var):\n",
    "        \"\"\"\n",
    "        This function is executed by each parallel worker. It sets up the\n",
    "        objective function for a specific outcome and runs the fmin loop.\n",
    "        \n",
    "        NOTE: Exceptions are no longer caught here - they will propagate and halt execution.\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        print(f\"[{start_time}] Starting optimization for outcome: {outcome_var}\", flush=True)\n",
    "\n",
    "        # Use functools.partial to bind the current outcome_var to the objective function.\n",
    "        fmin_objective = partial(objective, outcome_var=outcome_var)\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(\n",
    "            fn=fmin_objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=max_evals,\n",
    "            trials=trials,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        failed_trials = [t for t in trials.results if t['status'] == 'fail']\n",
    "        \n",
    "        print(f\"[{end_time}] Finished {outcome_var} (Duration: {end_time - start_time})\", flush=True)\n",
    "        print(f\"  -> Best param set for this outcome: {best}\", flush=True)\n",
    "        print(f\"  -> Trials summary: {len(failed_trials)}/{len(trials.results)} failed.\", flush=True)\n",
    "        \n",
    "        return (outcome_var, best)\n",
    "\n",
    "    # 8. --- Main Execution Block ---\n",
    "    num_cores = max(1, multiprocessing.cpu_count() - 2)\n",
    "    start_total = datetime.now()\n",
    "    print(f\"Starting all optimizations at {start_total} using {num_cores} cores.\")\n",
    "\n",
    "    # Parallel will raise exceptions from workers automatically\n",
    "    results = Parallel(n_jobs=num_cores, verbose=10)(\n",
    "        delayed(process_single_outcome)(outcome)\n",
    "        for outcome in outcome_var_list\n",
    "    )\n",
    "\n",
    "    # --- Process and display final results ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Hyperparameter Optimization Summary\")\n",
    "    print(\"=\"*70)\n",
    "    for outcome_var, best_params in results:\n",
    "        print(f\"\\n✅ Success for '{outcome_var}':\")\n",
    "        print(f\"   Best parameter combination found: {best_params}\")\n",
    "\n",
    "    end_total = datetime.now()\n",
    "    print(f\"\\nCompleted all optimizations at {end_total} (Total duration: {end_total - start_total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the parent directory from config\n",
    "parent_dir = experiment_dir # Use the path from the run\n",
    "\n",
    "# Check if the CSV is directly in the parent directory first\n",
    "csv_path = os.path.join(parent_dir, 'final_grid_score_log.csv')\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    # If not found directly, look for it in timestamped subfolders\n",
    "    # List all folders in the parent directory that match the date pattern\n",
    "    folders = [f for f in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, f))]\n",
    "\n",
    "    # Parse folder names as dates and find the latest one\n",
    "    def parse_date(folder_name: str):\n",
    "        \"\"\"\n",
    "        Parses the timestamp from the beginning of a folder name.\n",
    "        Expected format: 'YYYY-MM-DD_HH-MM-SS_...'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # The timestamp is always the first 19 characters.\n",
    "            timestamp_part = folder_name[:19]\n",
    "            return datetime.strptime(timestamp_part, '%Y-%m-%d_%H-%M-%S')\n",
    "        except (ValueError, IndexError):\n",
    "            # Return None if the folder name doesn't match the expected format or is too short.\n",
    "            return None\n",
    "\n",
    "    # Filter and sort folders by date\n",
    "    folders_with_dates = [(f, parse_date(f)) for f in folders]\n",
    "    folders_with_dates = [f for f in folders_with_dates if f[1] is not None]\n",
    "    \n",
    "    if folders_with_dates:\n",
    "        latest_folder = max(folders_with_dates, key=lambda x: x[1])[0]  # Get the folder with the latest date\n",
    "        print(\"latest_folder\", latest_folder)\n",
    "        \n",
    "        # Construct the path to the CSV file in the latest folder\n",
    "        csv_path = os.path.join(parent_dir, latest_folder, 'final_grid_score_log.csv')\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No timestamped folders found and CSV not in parent directory\")\n",
    "else:\n",
    "    print(\"CSV found directly in parent directory\")\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Sort the DataFrame by 'auc' column in descending order\n",
    "df = df.sort_values(by='auc', ascending=False)\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "\n",
    "# Group by outcome_variable and display the first row of each group with the highest auc\n",
    "df_best = df.groupby('outcome_variable').apply(lambda x: x.iloc[0])\n",
    "\n",
    "# Display the result\n",
    "df_best.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best['algorithm_implementation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Configuration ---\n",
    "experiments_base_dir = Path(config['experiment']['experiments_base_dir'])\n",
    "\n",
    "# --- Find the CSV file (try multiple locations) ---\n",
    "\n",
    "def find_csv_file():\n",
    "    \"\"\"\n",
    "    Search for final_grid_score_log.csv in multiple locations:\n",
    "    1. Project root (parent of experiments_base_dir)\n",
    "    2. Directly in experiments_base_dir\n",
    "    3. In the latest timestamped subfolder\n",
    "    \"\"\"\n",
    "    # Location 1: Project root (where the notebook is run from)\n",
    "    project_root = experiments_base_dir.parent\n",
    "    csv_path = project_root / 'final_grid_score_log.csv'\n",
    "    if csv_path.exists():\n",
    "        print(f\"✓ CSV found in project root: {csv_path.resolve()}\")\n",
    "        return csv_path\n",
    "    \n",
    "    # Location 2: Directly in experiments directory\n",
    "    csv_path = project_root / experiments_base_dir / 'final_grid_score_log.csv'\n",
    "    if csv_path.exists():\n",
    "        print(f\"✓ CSV found in experiments directory: {csv_path.resolve()}\")\n",
    "        return csv_path\n",
    "    \n",
    "    # Location 3: In latest timestamped subfolder\n",
    "    latest_folder = find_latest_experiment_folder()\n",
    "    if latest_folder:\n",
    "        csv_path = latest_folder / 'final_grid_score_log.csv' # latest_folder is already absolute\n",
    "        if csv_path.exists():\n",
    "            print(f\"✓ CSV found in latest experiment folder: {csv_path.resolve()}\")\n",
    "            return csv_path\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_date(folder_name: str):\n",
    "    \"\"\"\n",
    "    Parses the timestamp from the beginning of a folder name.\n",
    "    Expected format: 'YYYY-MM-DD_HH-MM-SS_...'.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        timestamp_part = folder_name[:19]\n",
    "        return datetime.strptime(timestamp_part, '%Y-%m-%d_%H-%M-%S')\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "\n",
    "def find_latest_experiment_folder():\n",
    "    \"\"\"Find the most recent timestamped experiment folder.\"\"\"\n",
    "    if not experiments_base_dir.exists() or not experiments_base_dir.is_dir():\n",
    "        print(f\"⚠ Experiments directory not found: {(project_root / experiments_base_dir).resolve()}\")\n",
    "        return None\n",
    "    \n",
    "    subfolders = [f for f in experiments_base_dir.iterdir() if f.is_dir()]\n",
    "    folders_with_dates = [(f, parse_date(f.name)) for f in subfolders]\n",
    "    valid_folders = [f for f in folders_with_dates if f[1] is not None]\n",
    "    \n",
    "    if valid_folders:\n",
    "        latest_folder = max(valid_folders, key=lambda x: x[1])[0]\n",
    "        print(f\"Latest experiment folder: {latest_folder.name}\")\n",
    "        return latest_folder\n",
    "    else:\n",
    "        print(\"⚠ No valid timestamped experiment folders found.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# Find the CSV file\n",
    "log_file_path = find_csv_file()\n",
    "\n",
    "if log_file_path and log_file_path.exists():\n",
    "    # Load the CSV file\n",
    "    df = pd.read_csv(log_file_path)\n",
    "    \n",
    "    # Sort by AUC in descending order\n",
    "    df_sorted = df.sort_values(by='auc', ascending=False)\n",
    "    \n",
    "    print(f\"\\n✓ Successfully loaded {len(df_sorted)} records from the log file.\")\n",
    "    \n",
    "    # Group by outcome_variable and get the best result for each\n",
    "    top_results_by_outcome = df_sorted.groupby('outcome_variable').first().reset_index()\n",
    "    \n",
    "    print(f\"\\nTop results by outcome variable ({len(top_results_by_outcome)} outcomes):\\n\")\n",
    "    \n",
    "    # Display the result\n",
    "    display(top_results_by_outcome)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n✗ Error: Could not find 'final_grid_score_log.csv' in any expected location:\")\n",
    "    print(f\"  - {(experiments_base_dir.parent / 'final_grid_score_log.csv').resolve()}\")\n",
    "    print(f\"  - {(project_root / experiments_base_dir / 'final_grid_score_log.csv').resolve()}\")\n",
    "    print(f\"  - In any timestamped subfolder within {experiments_base_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data_path = 'test_data_hfe_1yr_m_small_multiclass.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Shape of the dataset: {data.shape}\")\n",
    "print(f\"Columns: {data.columns.tolist()}\")\n",
    "print(\"\\nFirst 5 rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Check for constant features\n",
    "print(\"\\n=== Constant Features ===\")\n",
    "constant_features = [col for col in data.columns if data[col].nunique() == 1]\n",
    "print(f\"Constant features: {constant_features}\")\n",
    "\n",
    "# Check for features with very low variance (almost constant)\n",
    "print(\"\\n=== Low Variance Features ===\")\n",
    "low_variance_features = []\n",
    "for col in data.columns:\n",
    "    if data[col].dtype in [np.float64, np.int64]:  # Check only numeric features\n",
    "        if data[col].std() < 0.01:  # Threshold for low variance\n",
    "            low_variance_features.append(col)\n",
    "print(f\"Low variance features: {low_variance_features}\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\n=== Duplicate Rows ===\")\n",
    "duplicate_rows = data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for class distribution (if it's a classification problem)\n",
    "if 'target' in data.columns:  # Replace 'target' with your actual target column name\n",
    "    print(\"\\n=== Class Distribution ===\")\n",
    "    print(data['target'].value_counts())\n",
    "\n",
    "# Check for categorical features with high cardinality\n",
    "print(\"\\n=== High Cardinality Categorical Features ===\")\n",
    "categorical_features = data.select_dtypes(include=['object', 'category']).columns\n",
    "high_cardinality_features = [col for col in categorical_features if data[col].nunique() > 100]\n",
    "print(f\"High cardinality categorical features: {high_cardinality_features}\")\n",
    "\n",
    "# Check for outliers in numeric features (using IQR)\n",
    "print(\"\\n=== Outliers in Numeric Features ===\")\n",
    "numeric_features = data.select_dtypes(include=[np.float64, np.int64]).columns\n",
    "for col in numeric_features:\n",
    "    Q1 = data[col].quantile(0.25)\n",
    "    Q3 = data[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
    "    if not outliers.empty:\n",
    "        print(f\"Outliers in {col}: {len(outliers)} rows\")\n",
    "\n",
    "# Summary of issues\n",
    "print(\"\\n=== Summary of Issues ===\")\n",
    "if missing_values.any():\n",
    "    print(f\"- Missing values found in {missing_values[missing_values > 0].index.tolist()}\")\n",
    "if constant_features:\n",
    "    print(f\"- Constant features found: {constant_features}\")\n",
    "if low_variance_features:\n",
    "    print(f\"- Low variance features found: {low_variance_features}\")\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"- Duplicate rows found: {duplicate_rows}\")\n",
    "if high_cardinality_features:\n",
    "    print(f\"- High cardinality categorical features found: {high_cardinality_features}\")\n",
    "if not missing_values.any() and not constant_features and not low_variance_features and not duplicate_rows and not high_cardinality_features:\n",
    "    print(\"- No major issues found in the dataset.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "\n",
    "# Get the top result for each outcome_variable by AUC\n",
    "top_auc_per_outcome = df.loc[df.groupby('outcome_variable')['auc'].idxmax()]\n",
    "\n",
    "# Sort by AUC for better visualization\n",
    "top_auc_per_outcome = top_auc_per_outcome.sort_values(by='auc', ascending=False)\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create barplot to show the top AUC for each outcome_variable\n",
    "sns.barplot(\n",
    "    x='auc', \n",
    "    y='outcome_variable', \n",
    "    data=top_auc_per_outcome, \n",
    "    hue='nb_size', \n",
    "    dodge=False, \n",
    "    palette='viridis'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Top AUC for Each Outcome Variable')\n",
    "plt.xlabel('AUC')\n",
    "plt.ylabel('Outcome Variable')\n",
    "plt.legend(title='num features')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes\n",
    "from ml_grid.results_processing.core import ResultsAggregator\n",
    "from ml_grid.results_processing.plot_master import MasterPlotter\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load your data using the ResultsAggregator\n",
    "#    Replace with the actual path to your results and feature names file.\n",
    "#    The feature_names_csv is optional but required for feature-related plots.\n",
    "try:\n",
    "    aggregator = ResultsAggregator(\n",
    "        root_folder=config['experiment']['experiments_base_dir'],\n",
    "        feature_names_csv=config['data']['file_path'])\n",
    "    results_df = aggregator.aggregate_all_runs()\n",
    "\n",
    "    # 2. Instantiate the MasterPlotter with your data\n",
    "    master_plotter = MasterPlotter(results_df)\n",
    "\n",
    "    # 3. Call the plot_all() method to generate all visualizations\n",
    "    #    You can customize the primary metric and other options.\n",
    "    master_plotter.plot_all(metric='auc', stratify_by_outcome=True)\n",
    "\n",
    "except (ValueError, FileNotFoundError) as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    print(\"Please ensure your results folder path is correct and contains valid run data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_grid_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
